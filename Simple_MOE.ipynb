{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm  #Â colormaps\n",
    "                                        \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models, Model \n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import BatchNormalization, Layer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.losses import categorical_crossentropy, mse\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray([[1.,0.], [-1.,0.], [0.,1.],[0.,-1.]]).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x141ad8410>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAXC0lEQVR4nO3dYYwU533H8e+vh8FUVQqYk4vPyOCG2HHkCtItjYqUNA42xC981HESXEXBqSOaNGnVRrEC8otUTiyT+oWjtlYT5BKTNLLdEodc5FgUg928CQ6LIAY7wpxxW3Mm5mqMpcpXjPG/L/a5aDh2726Zub07P7+PtNqZ53lm9s/sMr+bmb0bRQRmZpav35jsAszMbHI5CMzMMucgMDPLnIPAzCxzDgIzs8zNmOwCLsT8+fNj0aJFk12Gmdm0sm/fvv+JiO6R7dMyCBYtWkS9Xp/sMszMphVJ/9Ws3aeGzMwy5yAwM8ucg8DMLHMOAjOzzDkIzMwyV0kQSNoi6YSkQy36JenvJfVLekbS+wt96yQdSY91VdRj1mnb9w+wYtNuFm94jBWbdrN9/8Bkl2Q2blUdETwIrB6l/6PAkvRYD/wTgKR5wFeBPwSWA1+VNLeimsw6Yvv+ATY+epCBU0MEMHBqiI2PHnQY2LRRSRBExE+Bk6MM6QW+Gw17gDmSFgCrgJ0RcTIiXgN2MnqgmE059+44zNCZs+e0DZ05y707Dk9SRWbt6dQ1gh7gpcL8sdTWqv08ktZLqkuqDw4OTlihZu16+dRQW+1mU820uVgcEZsjohYRte7u835D2mzSXDZndlvtZlNNp4JgAFhYmL88tbVqN5s27lh1FbMv6jqnbfZFXdyx6qpJqsisPZ0Kgj7g0+nbQx8AXo+I48AO4AZJc9NF4htSm9m0sWZZD/fcfC09c2YjoGfObO65+VrWLGt6ltNsyqnkj85Jegj4Y2C+pGM0vgl0EUBEfAv4CXAj0A+8AXwm9Z2U9DVgb1rVXREx2kVnsylpzbIe7/ht2qokCCLi1jH6A/hCi74twJYq6jAzs/ZNm4vFZmY2MRwEZmaZcxCYmWXOQWBmljkHgZlZ5hwEZmaZcxCYmWXOQWBmljkHgZlZ5hwEZmaZcxCYmWXOQWBmljkHgZlZ5hwEZmaZcxCYmWXOQWBmlrlKgkDSakmHJfVL2tCk/z5JB9LjeUmnCn1nC319VdRjZmbjV/oOZZK6gPuB64FjwF5JfRHx3PCYiPibwvi/BJYVVjEUEUvL1mFmZhemiiOC5UB/RByNiDeBh4HeUcbfCjxUweuamVkFqgiCHuClwvyx1HYeSVcAi4HdheaLJdUl7ZG0ptWLSFqfxtUHBwcrKNvMzKDzF4vXAtsi4myh7YqIqAF/CnxT0u82WzAiNkdELSJq3d3dnajVzCwLVQTBALCwMH95amtmLSNOC0XEQHo+CjzFudcPzMxsglURBHuBJZIWS5pJY2d/3rd/JF0NzAV+VmibK2lWmp4PrACeG7msmZlNnNLfGoqItyR9EdgBdAFbIuJZSXcB9YgYDoW1wMMREYXF3wt8W9LbNEJpU/HbRmZmNvF07n55eqjValGv1ye7DDOzaUXSvnRN9hz+zWIzs8w5CMzMMucgMDPLnIPAzCxzDgIzs8w5CMzMMucgMDPLnIPAzCxzDgIzs8w5CMzMMucgMDPLnIPAzCxzDgIzs8w5CMzMMucgMDPLnIPAzCxzlQSBpNWSDkvql7ShSf9tkgYlHUiPzxb61kk6kh7rqqjHzMzGr/StKiV1AfcD1wPHgL2S+prccvKRiPjiiGXnAV8FakAA+9Kyr5Wty8zMxqeKI4LlQH9EHI2IN4GHgd5xLrsK2BkRJ9POfyewuoKazMxsnKoIgh7gpcL8sdQ20sckPSNpm6SFbS6LpPWS6pLqg4ODFZRtZmbQuYvFPwYWRcTv0fipf2u7K4iIzRFRi4had3d35QWameWqiiAYABYW5i9Pbb8WEa9GxOk0+wDw++Nd1szMJlYVQbAXWCJpsaSZwFqgrzhA0oLC7E3AL9P0DuAGSXMlzQVuSG1mZtYhpb81FBFvSfoijR14F7AlIp6VdBdQj4g+4K8k3QS8BZwEbkvLnpT0NRphAnBXRJwsW5OZmY2fImKya2hbrVaLer0+2WWYmU0rkvZFRG1ku3+z2Mwscw4CM7PMOQjMzDLnIDAzy5yDwMwscw4CM7PMOQjMzDLnIDAzy5yDwMwscw4CM7PMOQjMzDLnIDAzy5yDwMwscw4CM7PMOQjMzDJXSRBIWi3psKR+SRua9H9J0nPp5vW7JF1R6Dsr6UB69I1c1szMJlbpO5RJ6gLuB64HjgF7JfVFxHOFYfuBWkS8IenzwN8Bn0x9QxGxtGwdZmZ2Yao4IlgO9EfE0Yh4E3gY6C0OiIgnI+KNNLuHxk3qzcxsCqgiCHqAlwrzx1JbK7cDjxfmL5ZUl7RH0ppWC0lan8bVBwcHy1VsZma/VvrUUDskfQqoAR8qNF8REQOSrgR2SzoYES+MXDYiNgOboXHP4o4UbGaWgSqOCAaAhYX5y1PbOSStBO4EboqI08PtETGQno8CTwHLKqjJzMzGqYog2AsskbRY0kxgLXDOt38kLQO+TSMEThTa50qalabnAyuA4kVmMzObYKVPDUXEW5K+COwAuoAtEfGspLuAekT0AfcCvwX8mySA/46Im4D3At+W9DaNUNo04ttGZmY2wRQx/U6312q1qNfrk12Gmdm0ImlfRNRGtvs3i83MMucgMDPLnIPAzCxzDgIzs8w5CMzMMucgMDPLnIPAzCxzDgIzs8w5CMzMMucgMDPLnIPAzCxzDgIzs8w5CMzMMucgMDPLnIPAzCxzDgIzs8xVEgSSVks6LKlf0oYm/bMkPZL6n5a0qNC3MbUflrSqinqa2b5/gBWbdrN4w2Os2LSb7fvPu62ymdmUNNH7r9K3qpTUBdwPXA8cA/ZK6htxy8nbgdci4t2S1gLfAD4p6Roa9zh+H3AZ8ISk90TE2bJ1FW3fP8DGRw8ydKax2oFTQ2x89CAAa5b1VPlSZmaV6sT+q4ojguVAf0QcjYg3gYeB3hFjeoGtaXob8BE1bl7cCzwcEacj4kWgP62vUvfuOPzrjThs6MxZ7t1xuOqXMjOrVCf2X1UEQQ/wUmH+WGprOiYi3gJeBy4Z57IASFovqS6pPjg42FaBL58aaqvdzGyq6MT+a9pcLI6IzRFRi4had3d3W8teNmd2W+1mZlNFJ/ZfVQTBALCwMH95ams6RtIM4LeBV8e5bGl3rLqK2Rd1ndM2+6Iu7lh1VdUvZWZWqU7sv6oIgr3AEkmLJc2kcfG3b8SYPmBdmr4F2B0RkdrXpm8VLQaWAD+voKZzrFnWwz03X0vPnNkI6Jkzm3tuvtYXis1syuvE/kuN/XHJlUg3At8EuoAtEXG3pLuAekT0SboY+B6wDDgJrI2Io2nZO4E/A94C/joiHh/r9Wq1WtTr9dJ1m5nlRNK+iKid115FEHSag8DMrH2tgmDaXCw2M7OJ4SAwM8ucg8DMLHMOAjOzzDkIzMwy5yAwM8ucg8DMLHMOAjOzzDkIzMwy5yAwM8ucg8DMLHMOAjOzzDkIzMwy5yAwM8ucg8DMLHMOAjOzzJUKAknzJO2UdCQ9z20yZqmkn0l6VtIzkj5Z6HtQ0ouSDqTH0jL1mJlZ+8oeEWwAdkXEEmBXmh/pDeDTEfE+YDXwTUlzCv13RMTS9DhQsh4zM2tT2SDoBbam6a3AmpEDIuL5iDiSpl8GTgDdJV/XzMwqUjYILo2I42n6V8Clow2WtByYCbxQaL47nTK6T9KsUZZdL6kuqT44OFiybDMzGzZmEEh6QtKhJo/e4riICCBGWc8C4HvAZyLi7dS8Ebga+ANgHvCVVstHxOaIqEVErbvbBxRmZlWZMdaAiFjZqk/SK5IWRMTxtKM/0WLcu4DHgDsjYk9h3cNHE6clfQf4clvVm5lZaWVPDfUB69L0OuBHIwdImgn8EPhuRGwb0bcgPYvG9YVDJesxM7M2lQ2CTcD1ko4AK9M8kmqSHkhjPgF8ELityddEvy/pIHAQmA98vWQ9ZmbWJjVO7U8vtVot6vX6ZJdhZjatSNoXEbWR7f7NYjOzzDkIzMwy5yAwM8ucg8DMLHMOAjOzzDkIzMwy5yAwM8ucg8DMLHMOAjOzzDkIzMwy5yAwM8ucg8DMLHMOAjOzzDkIzMwy5yAwM8tcqSCQNE/STklH0vPcFuPOFm5K01doXyzpaUn9kh5JdzMzM7MOKntEsAHYFRFLgF1pvpmhiFiaHjcV2r8B3BcR7wZeA24vWY+ZmbWpbBD0AlvT9FYa9x0el3Sf4uuA4fsYt7W8mZlVo2wQXBoRx9P0r4BLW4y7WFJd0h5Jwzv7S4BTEfFWmj8G9LR6IUnr0zrqg4ODJcs2M7NhM8YaIOkJ4HeadN1ZnImIkNTqBshXRMSApCuB3emG9a+3U2hEbAY2Q+Oexe0sa2ZmrY0ZBBGxslWfpFckLYiI45IWACdarGMgPR+V9BSwDPgBMEfSjHRUcDkwcAH/BjMzK6HsqaE+YF2aXgf8aOQASXMlzUrT84EVwHMREcCTwC2jLW9mZhOrbBBsAq6XdARYmeaRVJP0QBrzXqAu6Rc0dvybIuK51PcV4EuS+mlcM/jnkvWYmVmb1PjBfHqp1WpRr9cnuwwzs2lF0r6IqI1s928Wm5llzkFgZpY5B4GZWeYcBGZmmXMQmJllzkFgZpY5B4GZWeYcBGZmmXMQmJllzkFgZpY5B4GZWeYcBGZmmXMQmJllzkFgZpY5B4GZWeYcBGZmmSsVBJLmSdop6Uh6nttkzIclHSg8/k/SmtT3oKQXC31Ly9RjZmbtK3tEsAHYFRFLgF1p/hwR8WRELI2IpcB1wBvAvxeG3DHcHxEHStZjZmZtKhsEvcDWNL0VWDPG+FuAxyPijZKva2ZmFSkbBJdGxPE0/Svg0jHGrwUeGtF2t6RnJN0naVarBSWtl1SXVB8cHCxRspmZFY0ZBJKekHSoyaO3OC4iAohR1rMAuBbYUWjeCFwN/AEwD/hKq+UjYnNE1CKi1t3dPVbZZmY2TjPGGhARK1v1SXpF0oKIOJ529CdGWdUngB9GxJnCuoePJk5L+g7w5XHWbWZmFSl7aqgPWJem1wE/GmXsrYw4LZTCA0micX3hUMl6zMysTWWDYBNwvaQjwMo0j6SapAeGB0laBCwE/mPE8t+XdBA4CMwHvl6yHjMza9OYp4ZGExGvAh9p0l4HPluY/0+gp8m468q8vpmZleffLDYzy5yDwMwscw4CM7PMOQjMzDLnIDAzy5yDwMwscw4CM7PMOQjMzDLnIDAzy5yDwMwscw4CM7PMOQjMzDLnIDAzy5yDwMwscw4CM7PMOQjMzDJXKggkfVzSs5LellQbZdxqSYcl9UvaUGhfLOnp1P6IpJll6jGbLNv3D7Bi024Wb3iMFZt2s33/wGSXZDZuZY8IDgE3Az9tNUBSF3A/8FHgGuBWSdek7m8A90XEu4HXgNtL1mPWcdv3D7Dx0YMMnBoigIFTQ2x89KDDwKaNUkEQEb+MiMNjDFsO9EfE0Yh4E3gY6E03rL8O2JbGbaVxA3uzaeXeHYcZOnP2nLahM2e5d8dY/zXMpoZOXCPoAV4qzB9LbZcApyLirRHtTUlaL6kuqT44ODhhxZq16+VTQ221m001YwaBpCckHWry6O1EgcMiYnNE1CKi1t3d3cmXNhvVZXNmt9VuNtXMGGtARKws+RoDwMLC/OWp7VVgjqQZ6ahguN1sWrlj1VVsfPTgOaeHZl/UxR2rrprEqszGrxOnhvYCS9I3hGYCa4G+iAjgSeCWNG4d8KMO1GNWqTXLerjn5mvpmTMbAT1zZnPPzdeyZlnLM51mU4oa++MLXFj6E+AfgG7gFHAgIlZJugx4ICJuTONuBL4JdAFbIuLu1H4ljYvH84D9wKci4vRYr1ur1aJer19w3WZmOZK0LyLO+6p/qSCYLA4CM7P2tQoC/2axmVnmHARmZplzEJiZZc5BYGaWuWl5sVjSIPBfF7j4fOB/KiynKq6rPa6rPa6rPe/Uuq6IiPN+I3daBkEZkurNrppPNtfVHtfVHtfVntzq8qkhM7PMOQjMzDKXYxBsnuwCWnBd7XFd7XFd7cmqruyuEZiZ2blyPCIwM7MCB4GZWebekUEg6eOSnpX0tqSWX7WStFrSYUn9kjYU2hdLejq1P5L+fHYVdc2TtFPSkfQ8t8mYD0s6UHj8n6Q1qe9BSS8W+pZ2qq407mzhtfsK7ZO5vZZK+ll6v5+R9MlCX6Xbq9XnpdA/K/37+9P2WFTo25jaD0taVaaOC6jrS5KeS9tnl6QrCn1N39MO1XWbpMHC63+20Lcuve9HJK3rcF33FWp6XtKpQt+EbC9JWySdkHSoRb8k/X2q+RlJ7y/0ld9WEfGOewDvBa4CngJqLcZ0AS8AVwIzgV8A16S+fwXWpulvAZ+vqK6/Azak6Q3AN8YYPw84Cfxmmn8QuGUCtte46gL+t0X7pG0v4D3AkjR9GXAcmFP19hrt81IY8xfAt9L0WuCRNH1NGj8LWJzW09XBuj5c+Ax9friu0d7TDtV1G/CPTZadBxxNz3PT9NxO1TVi/F/S+NP5E729Pgi8HzjUov9G4HFAwAeAp6vcVu/II4KI+GVEjHXn8OVAf0QcjYg3adwXoVeSgOuAbWncVmBNRaX1pvWNd723AI9HxBsVvX4r7db1a5O9vSLi+Yg4kqZfBk7QuD9G1Zp+XkapdxvwkbR9eoGHI+J0RLwI9Kf1daSuiHiy8BnaQ+NugBNtPNurlVXAzog4GRGvATuB1ZNU163AQxW9dksR8VMaP/S10gt8Nxr20Li74wIq2lbvyCAYpx7gpcL8sdR2CXAqGrfPLLZX4dKIOJ6mfwVcOsb4tZz/Ibw7HRreJ2lWh+u6WFJd0p7h01VMoe0laTmNn/JeKDRXtb1afV6ajknb43Ua22c8y05kXUW30/jJcliz97STdX0svT/bJA3f0nZKbK90Cm0xsLvQPFHbayyt6q5kW415z+KpStITwO806bozIibtlpej1VWciYiQ1PK7uyntrwV2FJo30tghzqTxfeKvAHd1sK4rImJAjTvL7ZZ0kMbO7oJVvL2+B6yLiLdT8wVvr3ciSZ8CasCHCs3nvacR8ULzNVTux8BDEXFa0p/TOJq6rkOvPR5rgW0RcbbQNpnba8JM2yCIiJUlVzEALCzMX57aXqVx2DUj/VQ33F66LkmvSFoQEcfTjuvEKKv6BPDDiDhTWPfwT8enJX0H+HIn64qIgfR8VNJTwDLgB0zy9pL0LuAxGj8E7Cms+4K3VxOtPi/NxhyTNAP4bRqfp/EsO5F1IWkljXD9UBRuB9viPa1ixzZmXRHxamH2ARrXhIaX/eMRyz5VQU3jqqtgLfCFYsMEbq+xtKq7km2V86mhvcASNb7xMpPGm94XjSswT9I4Pw+wDqjqCKMvrW886z3v3GTaGQ6fl18DNP2GwUTUJWnu8KkVSfOBFcBzk7290nv3QxrnT7eN6KtyezX9vIxS7y3A7rR9+oC1anyraDGwBPh5iVraqkvSMuDbwE0RcaLQ3vQ97WBdCwqzNwG/TNM7gBtSfXOBGzj3yHhC60q1XU3j4uvPCm0Tub3G0gd8On176APA6+kHnWq21URcAZ/sB/AnNM6VnQZeAXak9suAnxTG3Qg8TyPR7yy0X0njP2o/8G/ArIrqugTYBRwBngDmpfYa8EBh3CIaSf8bI5bfDRyksUP7F+C3OlUX8EfptX+Rnm+fCtsL+BRwBjhQeCydiO3V7PNC41TTTWn64vTv70/b48rCsnem5Q4DH6348z5WXU+k/wfD26dvrPe0Q3XdAzybXv9J4OrCsn+WtmM/8JlO1pXm/xbYNGK5CdteNH7oO54+y8doXMv5HPC51C/g/lTzQQrfhqxiW/lPTJiZZS7nU0NmZoaDwMwsew4CM7PMOQjMzDLnIDAzy5yDwMwscw4CM7PM/T9TiS/oPCKXjAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X[:,0], X[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.asarray([0.,1.,1.,0.]).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_onehot = to_categorical(y)\n",
    "y_onehot.shape\n",
    "y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_onehot = np.asarray([[1., 0.],\n",
    "                       [0., 1.],\n",
    "                       [0., 1.],\n",
    "                       [1., 0.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = input = layers.Input(shape=(2,), name='myInput')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The selector layer that combines the expert and gate outputs. It is a non trainable layer\n",
    "class selector_expectation(Layer):\n",
    "    def __init__(self,num_experts, **kwargs):\n",
    "        self.num_experts = num_experts\n",
    "        super(selector_expectation,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.batch_size = input_shape[0]\n",
    "        super(selector_expectation, self).build(input_shape)\n",
    "        \n",
    "    def call(self,input):\n",
    "\n",
    "        x = input[0]\n",
    "        p = input[1]\n",
    "#         tf.print(self.batch_size, type(self.batch_size))\n",
    "        tf.print('x:',x)\n",
    "        tf.print(K.shape(x))\n",
    "        tf.print('p:',p,'\\n')\n",
    "        x = K.reshape(x,(batch_size, self.num_experts, 2))\n",
    "        tf.print(K.shape(x))\n",
    "        tf.print(x)\n",
    "        output =  K.sum(K.repeat_elements(K.reshape(p,(batch_size,self.num_experts,1)), rep=2,axis=2) * x, axis=1)\n",
    "        tf.print('o:', output)\n",
    "#         output = K.reshape(output, (1,2))\n",
    "        return output\n",
    "        \n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return (input_shape[0],1,input_shape[-1])\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(selector_expectation,self).get_config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Expert network\n",
    "def expert_layers(input, name):\n",
    "    exp_wts_mat = {'layer_0_wts': np.random.randn(2,X.shape[1]).astype(\"float32\"),\n",
    "                'layer_1_wts': np.random.randn(2,X.shape[1]).astype(\"float32\"),}\n",
    "    exp_bias_mat = {'layer_0_bias': np.random.randn(2,1),\n",
    "                 'layer_1_bias': np.random.randn(2,1)}\n",
    "    x = layers.Dense(2,activation='relu',\n",
    "                    kernel_initializer=tf.constant_initializer(exp_wts_mat['layer_0_wts']), \n",
    "                     bias_initializer=tf.constant_initializer(exp_bias_mat['layer_0_bias']))(input)\n",
    "    x = layers.Dense(2, activation='softmax',name=name,\n",
    "                    kernel_initializer=tf.constant_initializer(exp_wts_mat['layer_1_wts']), \n",
    "                     bias_initializer=tf.constant_initializer(exp_bias_mat['layer_1_bias']))(x)\n",
    "    return x, exp_wts_mat, exp_bias_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "gate_wts_mat = {'layer_0_wts': np.random.randn(2,X.shape[1]).astype(\"float32\"),\n",
    "                'layer_1_wts': np.random.randn(2,X.shape[1]).astype(\"float32\"),}\n",
    "gate_bias_mat = {'layer_0_bias': np.random.randn(2,1),\n",
    "                 'layer_1_bias': np.random.randn(2,1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gate network (Similar to the expert layer)\n",
    "def gate_layers(input, name):    \n",
    "    x = layers.Dense(2, activation='relu', \n",
    "                    kernel_initializer=tf.constant_initializer(gate_wts_mat['layer_0_wts']), \n",
    "                    bias_initializer=tf.constant_initializer(gate_bias_mat['layer_0_bias']))(input)\n",
    "    x = layers.Dense(2,name=name,activation='softmax',\n",
    "                     kernel_initializer=tf.constant_initializer(gate_wts_mat['layer_1_wts']), \n",
    "                     bias_initializer=tf.constant_initializer(gate_bias_mat['layer_1_bias']))(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Model to intercept training to analyze the gradients during training\n",
    "class CustomModel(Model):\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)  # Forward pass\n",
    "            loss = self.compiled_loss(y, y_pred)\n",
    "        \n",
    "        trainable_vars = self.trainable_variables\n",
    "        trainable_vars_names = []\n",
    "        for var in trainable_vars:\n",
    "            trainable_vars_names.append(var.name.split(':')[0])\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "#         for i in range(len(gradients)):\n",
    "#             tf.print(trainable_vars_names[i], ':', gradients[i],'\\n')\n",
    "        return super(CustomModel, self).train_step(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experts = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize gate network\n",
    "gate = gate_layers(input, 'gate_layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize expert networks\n",
    "experts = []\n",
    "expert_wts = {}\n",
    "expert_bias = {}\n",
    "for i in range(num_experts):\n",
    "    exp_layers, exp_wts_mat, exp_bias_mat = expert_layers(input, 'expert_'+str(i))\n",
    "    experts.append(exp_layers)\n",
    "    expert_wts['expert_'+str(i)] = exp_wts_mat\n",
    "    expert_bias['expert_'+str(i)] = exp_bias_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the selector network\n",
    "expert_output = experts[0]\n",
    "if num_experts > 1:\n",
    "    expert_output = tf.keras.layers.Concatenate(axis=0)(experts)\n",
    "selector_output = selector_expectation(num_experts=num_experts)([expert_output, gate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_model = CustomModel(input, selector_output,name='selector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"selector\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "myInput (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            6           myInput[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            6           myInput[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "expert_0 (Dense)                (None, 2)            6           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "expert_1 (Dense)                (None, 2)            6           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2)            6           myInput[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 2)            0           expert_0[0][0]                   \n",
      "                                                                 expert_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "gate_layer (Dense)              (None, 2)            6           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "selector_expectation (selector_ (4, 2)               0           concatenate[0][0]                \n",
      "                                                                 gate_layer[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 36\n",
      "Trainable params: 36\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "selector_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(actual, pred):\n",
    "    loss = categorical_crossentropy(actual, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_model.compile(optimizer='sgd', loss=\"categorical_crossentropy\", metrics=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_layer_outputs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing output of expert 0\n",
    "\n",
    "exp_outputs = {}\n",
    "\n",
    "exp_wts_mat = expert_wts['expert_0'] \n",
    "exp_bias_mat = expert_bias['expert_0']\n",
    "\n",
    "# layer 0 - ReLU\n",
    "print('Expert 0 feed forward computation\\n')\n",
    "print('X', X[:1] )\n",
    "exp_wts_mat_t = np.transpose(exp_wts_mat['layer_0_wts'])\n",
    "print('W11:', exp_wts_mat_t[0])\n",
    "print('b11:', exp_bias_mat['layer_0_bias'][0])\n",
    "y1 = np.dot(X[:1], exp_wts_mat_t[0])+exp_bias_mat['layer_0_bias'][0]\n",
    "print('y1:', y1)\n",
    "exp_outputs['y1'] = y1\n",
    "h11 = max([[0.], y1])\n",
    "print('h11', h11)\n",
    "exp_outputs['h11'] = h11\n",
    "print('W12_t:', exp_wts_mat_t[1])\n",
    "print('b12:', exp_bias_mat['layer_0_bias'][1])\n",
    "y2 = np.dot(X[:1], exp_wts_mat_t[1])+exp_bias_mat['layer_0_bias'][1]\n",
    "print('y2:', y2)\n",
    "h12 =  max([[0.], y2])\n",
    "print('h12',h12)\n",
    "exp_outputs['h12'] = h12\n",
    "X1 = np.asarray([h11, h12]).reshape((1,2))\n",
    "print('layer 0 output or layer 1 input', X1, '\\n')\n",
    "\n",
    "\n",
    "# layer 1 - Softmax\n",
    "print('X1', X1 )\n",
    "exp_wts_mat_t = np.transpose(exp_wts_mat['layer_1_wts'])\n",
    "print('W21:', exp_wts_mat_t[0])\n",
    "print('b21:', exp_bias_mat['layer_1_bias'][0])\n",
    "h21 = np.dot(X1, exp_wts_mat_t[0])+exp_bias_mat['layer_1_bias'][0]\n",
    "print('h21', h21)\n",
    "exp_outputs['h21'] = h21\n",
    "print('W22_t:', exp_wts_mat_t[1])\n",
    "print('b22:', exp_bias_mat['layer_1_bias'][1])\n",
    "h22 =  np.dot(X1, exp_wts_mat_t[1])+exp_bias_mat['layer_1_bias'][1]\n",
    "print('h22',h22)\n",
    "exp_outputs['h22'] = h22\n",
    "expert_1_output = softmax(np.concatenate((h21, h22)))\n",
    "print('layer 1 output or Expert 1 output', expert_1_output)\n",
    "\n",
    "network_layer_outputs['expert_0'] = exp_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[:1])\n",
    "print('Weights/Biases',selector_model.get_layer('dense_1').trainable_weights,'\\n')\n",
    "eo1 = selector_model.get_layer('dense_1')(X[:1])\n",
    "print('dense_1:',eo1,'\\n')\n",
    "print('Weights/Biases',selector_model.get_layer('expert_0').trainable_weights, '\\n')\n",
    "eo1 = selector_model.get_layer('expert_0')(eo1)\n",
    "print('expert_1_layer:', eo1,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing output of expert 2\n",
    "\n",
    "exp_outputs = {}\n",
    "\n",
    "exp_wts_mat = expert_wts['expert_1'] \n",
    "exp_bias_mat = expert_bias['expert_1']\n",
    "\n",
    "# layer 0 - ReLU\n",
    "print('Expert 2 feed forward computation\\n')\n",
    "print('X', X[:1] )\n",
    "exp_wts_mat_t = np.transpose(exp_wts_mat['layer_0_wts'])\n",
    "print('W11:', exp_wts_mat_t[0])\n",
    "print('b11:', exp_bias_mat['layer_0_bias'][0])\n",
    "y1 = np.dot(X[:1], exp_wts_mat_t[0])+exp_bias_mat['layer_0_bias'][0]\n",
    "exp_outputs['y1'] = y1\n",
    "h11 = max([[0.], y1])\n",
    "exp_outputs['h11'] = h11\n",
    "print('h11', h11,'\\n')\n",
    "print('W12:', exp_wts_mat_t[1])\n",
    "print('b12:', exp_bias_mat['layer_0_bias'][1])\n",
    "y2 = np.dot(X[:1], exp_wts_mat_t[1])+exp_bias_mat['layer_0_bias'][1]\n",
    "exp_outputs['y2'] = y2\n",
    "h12 =  max([[0.],y2])\n",
    "exp_outputs['h12'] = h12\n",
    "print('h12',h12)\n",
    "\n",
    "X1 = np.concatenate((h11, h12)).reshape((1,2))\n",
    "print('layer 0 output or layer 1 input', X1, '\\n')\n",
    "\n",
    "# layer 1 - Softmax\n",
    "print('X1', X1 )\n",
    "exp_wts_mat_t = np.transpose(exp_wts_mat['layer_1_wts'])\n",
    "print('W21:', exp_wts_mat_t[0])\n",
    "print('b21:', exp_bias_mat['layer_1_bias'][0])\n",
    "h21 = np.dot(X1, exp_wts_mat_t[0])+exp_bias_mat['layer_1_bias'][0]\n",
    "print('h21', h21,'\\n' )\n",
    "exp_outputs['h21'] = h21\n",
    "print('W22:', exp_wts_mat_t[1])\n",
    "print('b22:', exp_bias_mat['layer_1_bias'][1])\n",
    "h22 =  np.dot(X1, exp_wts_mat_t[1])+exp_bias_mat['layer_1_bias'][1]\n",
    "print('h22',h22)\n",
    "exp_outputs['h22'] = h22\n",
    "\n",
    "expert_2_output = softmax(np.concatenate((h21, h22)))\n",
    "print('layer 2 output or Expert 2 output', expert_2_output)\n",
    "\n",
    "network_layer_outputs['expert_1'] = exp_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[:1])\n",
    "print('Weights/Biases',selector_model.get_layer('dense_2').trainable_weights,'\\n')\n",
    "eo2 = selector_model.get_layer('dense_2')(X[:1])\n",
    "print('dense_2:',eo2,'\\n')\n",
    "print('Weights/Biases',selector_model.get_layer('expert_1').trainable_weights, '\\n')\n",
    "eo2 = selector_model.get_layer('expert_1')(eo2)\n",
    "print('expert_2_layer:', eo2,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed forward of gate network\n",
    "\n",
    "# Computing output of gate\n",
    "\n",
    "gate_outputs = {}\n",
    "\n",
    "# layer 0 - ReLU\n",
    "print('Gate feed forward computation\\n')\n",
    "print('X', X[:1] )\n",
    "gate_wts_mat_t = np.transpose(gate_wts_mat['layer_0_wts'])\n",
    "print('W11:', gate_wts_mat_t[0])\n",
    "print('b11:', gate_bias_mat['layer_0_bias'][0])\n",
    "y1 = np.dot(X[:1], gate_wts_mat_t[0])+gate_bias_mat['layer_0_bias'][0]\n",
    "gate_outputs['y1'] = y1\n",
    "h11 = max([[0.], y1])\n",
    "print('h11', h11)\n",
    "gate_outputs['h11'] = h11\n",
    "print('W12:', gate_wts_mat_t[1])\n",
    "print('b12:', gate_bias_mat['layer_0_bias'][1])\n",
    "y2 = np.dot(X[:1], gate_wts_mat_t[1])+gate_bias_mat['layer_0_bias'][1]\n",
    "gate_outputs['y2'] = y2\n",
    "h12 =  max([[0.],y2])\n",
    "print('h12',h12)\n",
    "gate_outputs['h12'] = h12\n",
    "X1 = np.concatenate((h11, h12)).reshape((1,2))\n",
    "print('layer 0 output or layer 1 input', X1, '\\n')\n",
    "\n",
    "\n",
    "# layer 1 - Softmax\n",
    "print('X1', X1 )\n",
    "gate_wts_mat_t = np.transpose(gate_wts_mat['layer_1_wts'])\n",
    "print('W21:', gate_wts_mat_t[0])\n",
    "print('b21:', gate_bias_mat['layer_1_bias'][0])\n",
    "h21 = np.dot(X1, gate_wts_mat_t[0])+gate_bias_mat['layer_1_bias'][0]\n",
    "print('h21', h21)\n",
    "gate_outputs['h21'] = h21\n",
    "print('W22:', gate_wts_mat_t[1])\n",
    "print('b22:', gate_bias_mat['layer_1_bias'][1])\n",
    "h22 =  np.dot(X1, gate_wts_mat_t[1])+gate_bias_mat['layer_1_bias'][1]\n",
    "print('h22',h22)\n",
    "gate_outputs['h22'] = h22\n",
    "print(np.concatenate((h21, h22)))\n",
    "gate_output = softmax(np.concatenate((h21, h22)))\n",
    "print('layer 1 output or Gate output', gate_output)\n",
    "\n",
    "network_layer_outputs['gate'] = gate_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[:1])\n",
    "print(selector_model.get_layer('dense').trainable_weights)\n",
    "go = selector_model.get_layer('dense')(X[:1])\n",
    "print('dense_0:',go,'\\n')\n",
    "print('Weights/Biases',selector_model.get_layer('gate_layer').trainable_weights, '\\n')\n",
    "go = selector_model.get_layer('gate_layer')(go)\n",
    "print('gate_layer:', go,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected output of experts using gate\n",
    "sel_out = np.add(np.multiply(gate_output[0], expert_1_output), np.multiply(gate_output[1], expert_2_output))\n",
    "sel_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expert 1 gradients\n",
    "t1 = y_onehot[0,0]\n",
    "t2 = y_onehot[0,1]\n",
    "\n",
    "p1 = gate_output[0]\n",
    "p2 = gate_output[1]\n",
    "o11 = expert_1_output[0]\n",
    "o12 = expert_1_output[1]\n",
    "\n",
    "exp_outputs = network_layer_outputs['expert_1']\n",
    "y1 = exp_outputs['y1']\n",
    "y2 = exp_outputs['y2']\n",
    "h11 = exp_outputs['h11']\n",
    "h12 = exp_outputs['h12']\n",
    "h21 = exp_outputs['h21']\n",
    "h22 = exp_outputs['h22']\n",
    "\n",
    "exp_wts_mat = expert_wts['expert_1'] \n",
    "exp_bias_mat = expert_bias['expert_1']\n",
    "\n",
    "exp_wts_mat_t = np.transpose(exp_wts_mat['layer_0_wts'])\n",
    "W11_0 = exp_wts_mat_t[0][0]\n",
    "W11_1 = exp_wts_mat_t[0][1]\n",
    "W12_0 = exp_wts_mat_t[1][0]\n",
    "W12_1 = exp_wts_mat_t[1][1]\n",
    "\n",
    "exp_wts_mat_t = np.transpose(exp_wts_mat['layer_1_wts'])\n",
    "W21_0 = exp_wts_mat_t[0][0]\n",
    "W21_1 = exp_wts_mat_t[0][1]\n",
    "W22_0 = exp_wts_mat_t[1][0]\n",
    "W22_1 = exp_wts_mat_t[1][1]\n",
    "\n",
    "B11 = exp_bias_mat['layer_0_bias'][0]\n",
    "B12 = exp_bias_mat['layer_0_bias'][1]\n",
    "B21 = exp_bias_mat['layer_1_bias'][0]\n",
    "B22 = exp_bias_mat['layer_1_bias'][1]\n",
    "\n",
    "\n",
    "# o1 = p1*o11 + p2*o21\n",
    "o1 = sel_out[0]\n",
    "# o2 = p1*o12 + p2*o22\n",
    "o2 = sel_out[1]\n",
    "\n",
    "# cross entropy loss\n",
    "# L = -t1 * log(o1) - t2 * log (o2)\n",
    "\n",
    "# dL/do11 = dL/do1 * do1/do11 \n",
    "# dL/do1 = -t1/o1\n",
    "# do1/do11 = p1\n",
    "dL_do11 = -1 * (t1/o1) * p1\n",
    "\n",
    "# dL/do12 = dL/do2 * do2/do12\n",
    "# dL/do2 = -t2/o2\n",
    "# do2/do12 = p1\n",
    "dL_do12 = -1. * (t2/o2) * p1\n",
    "\n",
    "# gradients for expert_1 layer kernel\n",
    "\n",
    "# h21 = h11*W21_0 + h12*W21_1 + B21\n",
    "# do11/dW21_0 = do11/dh21 * dh21/dW21_0 \n",
    "# do11/dh21 = o11(1-o11) derivative of softmax when i = j\n",
    "# dh21/dW21_0 = h11\n",
    "do11_dW21_0 = o11 * (1.-o11) * h11[0]\n",
    "\n",
    "# do12/dW21_0 = do12/dh21 * dh21/dW21_0 \n",
    "# do12/dh21 = -o11o12 derivative of softmax when i != j\n",
    "# dh21/dW21_0 = h11\n",
    "do12_dW21_0 = -1. * o11 * o12 * h11[0]\n",
    "\n",
    "# dL/dW21_0 = dL/do11 * do11/dW21_0 + dL/do12 * do12/dW21_0\n",
    "dL_dW21_0 = (dL_do11 * do11_dW21_0 )+ (dL_do12 * do12_dW21_0)\n",
    "\n",
    "# h21 = h11*W21_0 + h12*W21_1 + b21\n",
    "# do11/dW21_1 = do11/dh21 * dh21/dW21_1 \n",
    "# do11/dh21 = o11(1-o11) derivative of softmax when i = j\n",
    "# dh21/dW21_1 = h12\n",
    "do11_dW21_1 = o11 * (1.-o11) * h12[0]\n",
    "\n",
    "# do12/dW21_1 = do12/dh21 * dh21/dW21_1 \n",
    "# do12/dh21 = -o11o12 derivative of softmax when i != j\n",
    "# dh21/dW21_1 = h12\n",
    "do12_dW21_1 = -1. * o11 * o12 * h12[0]\n",
    "\n",
    "# dL/dW21_1 = dL/do11 * do11/dW21_1\n",
    "dL_dW21_1= (dL_do11 * do11_dW21_1 )+ (dL_do12 * do12_dW21_1)\n",
    "\n",
    "# h22 = h11*W22_0 + h12*W22_1 + b22\n",
    "# do11/dW22_0 = do11/dh22 * dh22/dW22_0 \n",
    "# do11/dh22 = -o11o12 derivative of softmax when i != j\n",
    "# dh22/dW22_0 = h11\n",
    "do11_dW22_0 = -1. *o11 * o12 * h11[0]\n",
    "\n",
    "# do12/dW22_0 = do12/dh22 * dh22/dW22_0\n",
    "# do12/dh22 = o12(1-o12) derivative of softmax when i = j\n",
    "# dh22/dW22_0 = h11\n",
    "do12_dW22_0 = o12 * (1.-o12) * h11[0]\n",
    "\n",
    "# dL/dW22_0 = dL/do11 * do11/dW21_0 + dL/do12 * do12/dW22_0\n",
    "dL_dW22_0 = (dL_do11 * do11_dW22_0 )+ (dL_do12 * do12_dW22_0)\n",
    "\n",
    "# h22 = h11*W22_0 + h12*W22_1 + b22\n",
    "# do11/dW22_1 = do11/dh22 * dh22/dW22_1 \n",
    "# do11/dh22 = -o11o12 derivative of softmax when i != j\n",
    "# dh22/dW22_1 = h12\n",
    "do11_dW22_1 = -1. * o11 * o12 * h12[0]\n",
    "\n",
    "# do12/dW22_1 = do12/dh22 * dh22/dW22_1 \n",
    "# do12/dh22 = o12(1-o12) derivative of softmax when i == j\n",
    "# dh22/dW22_1 = h12\n",
    "do12_dW22_1 = o12 * (1.-o12) * h12[0]\n",
    "\n",
    "# dL/dW21_1 = dL/do11 * do11/dW21_1 + dL_o12 * do12_dW22_1\n",
    "dL_dW22_1= (dL_do11 * do11_dW22_1 )+ (dL_do12 * do12_dW22_1)\n",
    "\n",
    "# gradients for expert 1 layer biases\n",
    "\n",
    "# h21 = h11*W21_0 + h12*W21_1 + b21\n",
    "# do11/dB21 = do11/dh21 * dh21/dB21 \n",
    "# do11/dh21 = o11(1-o11) derivative of softmax when i = j\n",
    "# dh21/dB21 = 1.\n",
    "do11_dB21 = o11 * (1.-o11) * 1.\n",
    "\n",
    "# do12/dB21 = do12/dh21 * dh21/dB21 \n",
    "# do12/dh21 = -o11o12 derivative of softmax when i != j\n",
    "# dh21/dB21 = 1.\n",
    "do12_dB21 = -1. * o11 * o12 * 1.\n",
    "\n",
    "# dL/dB21 = dL/dp1 * dp1/dB21 + dL/dp2 * dp2/dB21\n",
    "dL_dB21 = (dL_do11 * do11_dB21 )+ (dL_do12 * do12_dB21)\n",
    "\n",
    "# h22 = h11*W22_0 + h12*W22_1 + b22\n",
    "# do11/dB22 = do11/dh22 * dh22/dB22 \n",
    "# do11/dh22 = -o11o12 derivative of softmax when i != j\n",
    "# dh22/dB22 = 1.\n",
    "do11_dB22 = -1. * o11 * o12 * 1.\n",
    "\n",
    "# do12/dB22 = do12/dh22 * dh22/dB22 \n",
    "# do12/dh22 = o12(1-o12) derivative of softmax when i = j\n",
    "# dh22/dB22 = 1.\n",
    "do12_dB22 = o12 * (1.-o12) * 1.\n",
    "\n",
    "# dL/dB22 = dL/dp1 * dp1/dB22 + dL/dp2 * dp2/dB22\n",
    "dL_dB22 = (dL_do11 * do11_dB22 )+ (dL_do12 * do12_dB22)\n",
    "\n",
    "# gradients for dense 2 layer weights\n",
    "\n",
    "# h21 = h11*W21_0 + h12*W21_1 + b21\n",
    "# h22 = h11*W22_0 + h12*W22_1 + b22\n",
    "# do11/dh11 = do11/dh21 * dh21/dh11 + do11/dh22 * dh22/dh11\n",
    "# do11/dh21 = o11(1-o11) derivative of softmax when i = j\n",
    "# dh21/dh11 = W21_0\n",
    "# do11/dh22 = -o11o12 derivative of softmax when i != j\n",
    "# dh22/dh11 = W22_0\n",
    "do11_dh11 = (o11 * (1.-o11) * W21_0) + (-1. * o11 * o12 * W22_0)\n",
    "\n",
    "# do12/dh11 = do12/dh21 * dh21/dh11 + do12/dh22 * dh22/dh11\n",
    "# do12/dh21 = -o11o12 derivative of softmax when i != j\n",
    "# dh21/dh11 = W21_0\n",
    "# do12/dh22 = o12(1-o12)\n",
    "# dh22/dh11 = W22_0\n",
    "do12_dh11 = (-1. * o11 * o12 * W21_0) + (o12 * (1.-o12) * W22_0)\n",
    "\n",
    "# dL/dh11 = dL/dp1 * dp1/dh11 + dL/dp2 * dp2/dh11\n",
    "dL_dh11 = (dL_do11 * do11_dh11 )+ (dL_do12 * do12_dh11)\n",
    "\n",
    "# y1 = x1 * W11_0 + x2 * W11_1 +b11\n",
    "# dh11/dW11_0 = dh11/dy1 * dy1/dW11_0\n",
    "# dh11/dy1 = 0 if y1 < 0 and 1 if y1 > 0 derivative of relu\n",
    "# dy1/dW11_0 = x1\n",
    "dh11_dW11_0 = 0.\n",
    "if y1 >= 0 :\n",
    "    dh11_dW11_0 = 1.\n",
    "dh11_dW11_0 = dh11_dW11_0 * X[0,0]\n",
    "\n",
    "#dL/dW11_0 = dL/dh11 * dh11/dW11_0\n",
    "dL_dW11_0 = dL_dh11 * dh11_dW11_0\n",
    "\n",
    "# y1 = x1 * W11_0 + x2 * W11_1 +b11\n",
    "# dh11/dW11_1 = dh11/dy1 * dy1/dW11_1\n",
    "# dh11/dy1 = 0 if y1 < 0 and 1 if y1 >= 0 derivative of relu\n",
    "# dy1/dW11_1 = x2\n",
    "dh11_dW11_1 = 0.\n",
    "if y1 >= 0 :\n",
    "    dh11_dW11_1 = 1.\n",
    "dh11_dW11_1 = dh11_dW11_1 * X[0,1]\n",
    "\n",
    "#dL/dW11_0 = dL/dh11 * dh11/dW11_0\n",
    "dL_dW11_1 = dL_dh11 * dh11_dW11_1\n",
    "\n",
    "# h21 = h11*W21_0 + h12*W21_1 + b21\n",
    "# h22 = h11*W22_0 + h12*W22_1 + b22\n",
    "# do11/dh12 = do11/dh21 * dh21/dh12 + do11/dh22 * dh22/dh12\n",
    "# do11/dh21 = o11(1-o11) derivative of softmax when i = j\n",
    "# dh21/dh12 = W21_1\n",
    "# do11/dh22 = -o11o12 derivative of softmax when i != j\n",
    "# dh22/dh12 = W22_1\n",
    "do11_dh12 = (o11 * (1.-o11) * W21_1) + (-1. * o11 * o12 * W22_1)\n",
    "\n",
    "# do12/dh12 = do12/dh21 * dh21/dh12 + do12/dh22 * dh22/dh12\n",
    "# do12/dh21 = -o11o12 derivative of softmax when i != j\n",
    "# dh21/dh12 = W21_1\n",
    "# do12/dh22 = o12(1-o12)\n",
    "# dh22/dh12 = W22_1\n",
    "do12_dh12 = (-1. * o11 * o12 * W21_1) + (o12 * (1.-o12) * W22_1)\n",
    "\n",
    "# dL/dh12 = dL/dp1 * dp1/dh12 + dL/dp2 * dp2/dh12\n",
    "dL_dh12 = (dL_do11 * do11_dh12 )+ (dL_do12 * do12_dh12)\n",
    "\n",
    "# y2 = x1 * W12_0 + x2 * W12_1 +b12\n",
    "# dh12/dW12_0 = dh12/dy2 * dy2/dW12_0\n",
    "# dh12/dy2 = 0 if y2 < 0 and 1 if y2 >= 0 derivative of relu\n",
    "# dy2/dW12_0 = x1\n",
    "dh12_dW12_0 = 0.\n",
    "if y2 >= 0 :\n",
    "    dh12_dW12_0 = 1.\n",
    "dh12_dW12_0 = dh12_dW12_0 * X[0,0]\n",
    "\n",
    "#dL/dW12_0 = dL/dh12 * dh12/dW12_0\n",
    "dL_dW12_0 = dL_dh12 * dh12_dW12_0\n",
    "\n",
    "# y2 = x1 * W12_0 + x2 * W12_1 +b12\n",
    "# dh12/dW12_1 = dh12/dy2 * dy2/dW12_1\n",
    "# dh12/dy1 = 0 if y2 < 0 and 1 if y2 >= 0 derivative of relu\n",
    "# dy2/dW12_1 = x2\n",
    "dh12_dW12_1 = 0.\n",
    "if y2 >= 0 :\n",
    "    dh12_dW12_1 = 1.\n",
    "dh12_dW12_1 = dh12_dW12_1 * X[0,1]\n",
    "\n",
    "#dL/dW11_0 = dL/dh12 * dh11/dW11_0 \n",
    "dL_dW12_1 = dL_dh12 * dh12_dW12_1\n",
    "\n",
    "# gradients of dense 2 layer biases\n",
    "\n",
    "# # y1 = x1 * W11_0 + x2 * W11_1 +b11\n",
    "# # dh11/dB11 = dh11/dy1 * dy1/dB11\n",
    "# # dh11/dy1 = 0 if h11 < 0 and 1 if h11 > 0 derivative of relu\n",
    "# # dy1/dB11 = 1.\n",
    "# dh11_dB11 = 0.\n",
    "# if h11[0] > 0 :\n",
    "#     dh11_dB11 = 1.\n",
    "# dh11_dB11 = dh11_dB11 * 1.\n",
    "\n",
    "# # y2 = x1 * W12_0 + x2 * W12_1 +b12\n",
    "# # dh12/dB12 = dh12/dy2 * dy2/dB12\n",
    "# # dh12/dy2 = 0 if h12 < 0 and 1 if h12 > 0 derivative of relu\n",
    "# # dy2/dB12 = 1.\n",
    "# dh12_dB12 = 0.\n",
    "# if h12[0] > 0 :\n",
    "#     dh12_dB12 = 1.\n",
    "# dh12_dB12 = dh12_dB12 * 1.\n",
    "\n",
    "# #dL/dB11 = dL/dp1 * dp1/dh11 * dh11/dB11 + dL/dp2 * dp2/dh11 * dh11/dB11\n",
    "# dL_dB11 = (dL_dp1 * dp1_dh11 * dh11_dB11)  + (dL_dp2 * dp2_dh11 * dh11_dB11)\n",
    "\n",
    "# #dL/dB12 = dL/dp1 * dp1/dh12 * dh12/dB12 + dL/dp2 * dp2/dh12 * dh12/dB12\n",
    "# dL_dB12= (dL_dp1 * dp1_dh12 * dh12_dB12)  + (dL_dp2 * dp2_dh12 * dh12_dB12)\n",
    "\n",
    "print('gradients for dense 2 layer kernel:\\n [[', dL_dW11_0 , dL_dW12_0, '],[',dL_dW11_1 , dL_dW12_1,']]')\n",
    "# # print('gradients for dense_layer bias:\\n [', dL_dB11,dL_dB12, ']')\n",
    "\n",
    "print('gradients for expert_1 kernel:\\n [[', dL_dW21_0 , dL_dW22_0, '],[',dL_dW21_1 , dL_dW22_1,']]')\n",
    "print('gradients for expert_1 layer bias:\\n [', dL_dB21,dL_dB22, ']')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_1/kernel : [[0.127086863 0.0918376222]\n",
    " [0 0]] \n",
    "\n",
    "dense_1/bias : [0.127086863 0.0918376222] \n",
    "\n",
    "dense_2/kernel : [[0.559607 1.46162128]\n",
    " [0 0]] \n",
    "\n",
    "dense_2/bias : [0.559607 1.46162128] \n",
    "\n",
    "expert_0/kernel : [[-0.262972623 0.262972593]\n",
    " [-0.0538177267 0.0538177229]] \n",
    "\n",
    "expert_0/bias : [-0.123351485 0.12335147] \n",
    "\n",
    "expert_1/kernel : [[-0.865493059 0.865493119]\n",
    " [-0.99379164 0.993791699]] \n",
    "\n",
    "expert_1/bias : [-0.717874348 0.717874408] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gate gradients\n",
    "t1 = y_onehot[0,0]\n",
    "t2 = y_onehot[0,1]\n",
    "\n",
    "p1 = gate_output[0]\n",
    "p2 = gate_output[1]\n",
    "o11 = expert_1_output[0]\n",
    "o12 = expert_1_output[1]\n",
    "o21 = expert_2_output[0]\n",
    "o22 = expert_2_output[1]\n",
    "\n",
    "gate_wts_mat_t = np.transpose(gate_wts_mat['layer_0_wts'])\n",
    "W11_0 = gate_wts_mat_t[0][0]\n",
    "W11_1 = gate_wts_mat_t[0][1]\n",
    "W12_0 = gate_wts_mat_t[1][0]\n",
    "W12_1 = gate_wts_mat_t[1][1]\n",
    "\n",
    "gate_wts_mat_t = np.transpose(gate_wts_mat['layer_1_wts'])\n",
    "W21_0 = gate_wts_mat_t[0][0]\n",
    "W21_1 = gate_wts_mat_t[0][1]\n",
    "W22_0 = gate_wts_mat_t[1][0]\n",
    "W22_1 = gate_wts_mat_t[1][1]\n",
    "\n",
    "B11 = gate_bias_mat['layer_0_bias'][0]\n",
    "B12 = gate_bias_mat['layer_0_bias'][1]\n",
    "B21 = gate_bias_mat['layer_1_bias'][0]\n",
    "B12 = gate_bias_mat['layer_1_bias'][1]\n",
    "\n",
    "gate_outputs = network_layer_outputs['gate']\n",
    "y1 = gate_outputs['y1']\n",
    "y2 = gate_outputs['y2']\n",
    "h11 = gate_outputs['h11']\n",
    "h12 = gate_outputs['h12']\n",
    "h21 = gate_outputs['h21']\n",
    "h22 = gate_outputs['h22']\n",
    "\n",
    "# o1 = p1*o11 + p2*o21\n",
    "o1 = sel_out[0]\n",
    "# o2 = p1*o12 + p2*o22\n",
    "o2 = sel_out[1]\n",
    "\n",
    "# cross entropy loss\n",
    "# L = -t1 * log(o1) - t2 * log (o2)\n",
    "# dL/dp1 = dL/do1 * do1/dp1 + dL/do2 * do2/dp1\n",
    "# dL/do1 = -t1/o1\n",
    "# do1/dp1 = o11\n",
    "# dL/do2 = -t2/o2\n",
    "# do2/dp1 = o12\n",
    "dL_dp1 = (-1. * (t1/o1) * o11) + (-1. * (t2/o2) * o12)\n",
    "# dL/dp2 = dL/do1 * do1/dp2 + dL/do2 * do2/dp2\n",
    "# dL/do1 = -t1/o1\n",
    "# do1/dp2 = o21\n",
    "# dL/do2 = -t2/o2\n",
    "# do2/dp2 = o22\n",
    "dL_dp2 = (-1. * (t1/o1) * o21) + (-1. * (t2/o2) * o22)\n",
    "\n",
    "# gradients for gate_layer weights\n",
    "\n",
    "# h21 = h11*W21_0 + h12*W21_1 + b21\n",
    "# dp1/dW21_0 = dp1/dh21 * dh21/dW21_0 \n",
    "# dp1/dh21 = p1(1-p1) derivative of softmax when i = j\n",
    "# dh21/dW21_0 = h11\n",
    "dp1_dW21_0 = p1 * (1.-p1) * h11[0]\n",
    "\n",
    "# dp2/dW21_0 = dp2/dh21 * dh21/dW21_0 \n",
    "# dp2/dh21 = -p1p2 derivative of softmax when i != j\n",
    "# dh21/dW21_0 = h11\n",
    "dp2_dW21_0 = -1. * p1 * p2 * h11[0]\n",
    "\n",
    "# dL/dW21_0 = dL/dp1 * dp1/dW21_0 + dL/dp2 * dp2/dW21_0\n",
    "dL_dW21_0 = (dL_dp1 * dp1_dW21_0 )+ (dL_dp2 * dp2_dW21_0)\n",
    "\n",
    "# h21 = h11*W21_0 + h12*W21_1 + b21\n",
    "# dp1/dW21_1 = dp1/dh21 * dh21/dW21_1 \n",
    "# dp1/dh21 = p1(1-p1) derivative of softmax when i = j\n",
    "# dh21/dW21_1 = h12\n",
    "dp1_dW21_1 = p1 * (1.-p1) * h12[0]\n",
    "\n",
    "# dp2/dW21_1 = dp2/dh21 * dh21/dW21_1 \n",
    "# dp2/dh21 = -p1p2 derivative of softmax when i != j\n",
    "# dh21/dW21_1 = h12\n",
    "dp2_dW21_1 = -1. * p1 * p2 * h12[0]\n",
    "\n",
    "# dL/dW21_1 = dL/dp1 * dp1/dW21_1\n",
    "dL_dW21_1= (dL_dp1 * dp1_dW21_1 )+ (dL_dp2 * dp2_dW21_1)\n",
    "\n",
    "# h22 = h11*W22_0 + h12*W22_1 + b22\n",
    "# dp1/dW22_0 = dp1/dh22 * dh22/dW22_0 \n",
    "# dp1/dh22 = -p1p2 derivative of softmax when i != j\n",
    "# dh22/dW22_0 = h11\n",
    "dp1_dW22_0 = -1. *p1 * p2 * h11[0]\n",
    "\n",
    "# dp2/dW22_0 = dp2/dh22 * dh22/dW22_0\n",
    "# dp2/dh22 = p1(1-p1) derivative of softmax when i = j\n",
    "# dh22/dW22_0 = h11\n",
    "dp2_dW22_0 = p1 * (1.-p1) * h11[0]\n",
    "\n",
    "# dL/dW21_0 = dL/dp1 * dp1/dW21_0\n",
    "dL_dW22_0 = (dL_dp1 * dp1_dW22_0 )+ (dL_dp2 * dp2_dW22_0)\n",
    "\n",
    "# h22 = h11*W22_0 + h12*W22_1 + b22\n",
    "# dp1/dW22_1 = dp1/dh22 * dh22/dW22_1 \n",
    "# dp1/dh22 = -p1p2 derivative of softmax when i != j\n",
    "# dh22/dW22_1 = h12\n",
    "dp1_dW22_1 = -1. * p1 * p2 * h12[0]\n",
    "\n",
    "# dp2/dW22_1 = dp2/dh22 * dh22/dW22_1 \n",
    "# dp2/dh22 = p1(1-p1) derivative of softmax when i == j\n",
    "# dh22/dW22_1 = h12\n",
    "dp2_dW22_1 = p1 * (1.-p1) * h12[0]\n",
    "\n",
    "# dL/dW21_1 = dL/dp1 * dp1/dW21_1\n",
    "dL_dW22_1= (dL_dp1 * dp1_dW22_1 )+ (dL_dp2 * dp2_dW22_1)\n",
    "\n",
    "# gradients for gate_layer biases\n",
    "\n",
    "# h21 = h11*W21_0 + h12*W21_1 + b21\n",
    "# dp1/dB21 = dp1/dh21 * dh21/dB21 \n",
    "# dp1/dh21 = p1(1-p1) derivative of softmax when i = j\n",
    "# dh21/dB21 = 1.\n",
    "dp1_dB21 = p1 * (1.-p1) * 1.\n",
    "\n",
    "# dp2/dB21 = dp2/dh21 * dh21/dB21 \n",
    "# dp2/dh21 = -p1p2 derivative of softmax when i != j\n",
    "# dh21/dB21 = 1.\n",
    "dp2_dB21 = -1. * p1 * p2 * 1.\n",
    "\n",
    "# dL/dB21 = dL/dp1 * dp1/dB21 + dL/dp2 * dp2/dB21\n",
    "dL_dB21 = (dL_dp1 * dp1_dB21 )+ (dL_dp2 * dp2_dB21)\n",
    "\n",
    "# h22 = h11*W22_0 + h12*W22_1 + b22\n",
    "# dp1/dB22 = dp1/dh22 * dh22/dB22 \n",
    "# dp1/dh22 = -p1p2 derivative of softmax when i != j\n",
    "# dh22/dB22 = 1.\n",
    "dp1_dB22 = -1. * p1 * p2 * 1.\n",
    "\n",
    "# dp2/dB22 = dp2/dh22 * dh22/dB22 \n",
    "# dp2/dh22 = p2(1-p2) derivative of softmax when i = j\n",
    "# dh22/dB22 = 1.\n",
    "dp2_dB22 = p2 * (1.-p2) * 1.\n",
    "\n",
    "# dL/dB22 = dL/dp1 * dp1/dB22 + dL/dp2 * dp2/dB22\n",
    "dL_dB22 = (dL_dp1 * dp1_dB22 )+ (dL_dp2 * dp2_dB22)\n",
    "\n",
    "# gradients for dense layer kernel\n",
    "\n",
    "# h21 = h11*W21_0 + h12*W21_1 + b21\n",
    "# h22 = h11*W22_0 + h12*W22_1 + b22\n",
    "# dp1/dh11 = dp1/dh21 * dh21/dh11 + dp1/dh22 * dh22/dh11\n",
    "# dp1/dh21 = p1(1-p1) derivative of softmax when i = j\n",
    "# dh21/dh11 = W21_0\n",
    "# dp1/dh22 = -p1p2 derivative of softmax when i != j\n",
    "# dh22/dh11 = W22_0\n",
    "dp1_dh11 = (p1 * (1.-p1) * W21_0) + (-1. * p1 * p2 * W22_0)\n",
    "\n",
    "# dp2/dh11 = dp2/dh21 * dh21/dh11 + dp2/dh22 * dh22/dh11\n",
    "# dp2/dh21 = -p1p2 derivative of softmax when i != j\n",
    "# dh21/dh11 = W21_0\n",
    "# dp2/dh22 = p2(1-p2)\n",
    "# dh22/dh11 = W22_0\n",
    "dp2_dh11 = (-1. * p1 * p2 * W21_0) + (p2 * (1.-p2) * W22_0)\n",
    "\n",
    "# dL/dh11 = dL/dp1 * dp1/dh11 + dL/dp2 * dp2/dh11\n",
    "dL_dh11 = (dL_dp1 * dp1_dh11 )+ (dL_dp2 * dp2_dh11)\n",
    "\n",
    "# y1 = x1 * W11_0 + x2 * W11_1 +b11\n",
    "y1 = X[0,0] * W11_0 + X[0,1] * W11_1 + B11\n",
    "# dh11/dW11_0 = dh11/dy1 * dy1/dW11_0\n",
    "# dh11/dy1 = 0 if y1 < 0 and 1 if y1 > 0 derivative of relu\n",
    "# dy1/dW11_0 = x1\n",
    "dh11_dW11_0 = 0.\n",
    "if y1 >= 0 :\n",
    "    dh11_dW11_0 = 1.\n",
    "dh11_dW11_0 = dh11_dW11_0 * X[0,0]\n",
    "\n",
    "#dL/dW11_0 = dL/dh11 * dh11/dW11_0 \n",
    "dL_dW11_0 = (dL_dh11 * dh11_dW11_0)\n",
    "\n",
    "# y1 = x1 * W11_0 + x2 * W11_1 +b11\n",
    "# dh11/dW11_1 = dh11/dy1 * dy1/dW11_1\n",
    "# dh11/dy1 = 0 if h11 < 0 and 1 if h11 > 0 derivative of relu\n",
    "# dy1/dW11_1 = x2\n",
    "dh11_dW11_1 = 0.\n",
    "if y1 >= 0 :\n",
    "    dh11_dW11_1 = 1.\n",
    "dh11_dW11_1 = dh11_dW11_1 * X[0,1]\n",
    "\n",
    "#dL/dW11_0 = dL/dh11 * dh11/dW11_0 + dL/dh11 * dh11/dW11_0\n",
    "dL_dW11_1 = dL_dh11 * dh11_dW11_1\n",
    "\n",
    "# h21 = h11*W21_0 + h12*W21_1 + b21\n",
    "# h22 = h11*W22_0 + h12*W22_1 + b22\n",
    "# dp1/dh12 = dp1/dh21 * dh21/dh12 + dp1/dh22 * dh22/dh12\n",
    "# dp1/dh21 = p1(1-p1) derivative of softmax when i = j\n",
    "# dh21/dh12 = W21_1\n",
    "# dp1/dh22 = -p1p2 derivative of softmax when i != j\n",
    "# dh22/dh12 = W22_1\n",
    "dp1_dh12 = (p1 * (1.-p1) * W21_1) + (-1. * p1 * p2 * W22_1)\n",
    "\n",
    "# dp2/dh12 = dp2/dh21 * dh21/dh12 + dp2/dh22 * dh22/dh12\n",
    "# dp2/dh21 = -p1p2 derivative of softmax when i != j\n",
    "# dh21/dh12 = W21_1\n",
    "# dp2/dh22 = p2(1-p2)\n",
    "# dh22/dh12 = W22_1\n",
    "dp2_dh12 = (-1. * p1 * p2 * W21_1) + (p2 * (1.-p2) * W22_1)\n",
    "\n",
    "# dL/dh12 = dL/dp1 * dp1/dh12 + dL/dp2 * dp2/dh12\n",
    "dL_dh12 = (dL_dp1 * dp1_dh12 )+ (dL_dp2 * dp2_dh12)\n",
    "\n",
    "# y2 = x1 * W12_0 + x2 * W12_1 +b12\n",
    "y2 = X[0,0] * W12_0 + X[0,1] * W12_1\n",
    "# dh12/dW12_0 = dh12/dy2 * dy2/dW12_0\n",
    "# dh12/dy2 = 0 if y2 < 0 and 1 if y2 >= 0 derivative of relu\n",
    "# dy2/dW12_0 = x1\n",
    "dh12_dW12_0 = 0.\n",
    "if y2 >= 0 :\n",
    "    dh12_dW12_0 = 1.\n",
    "dh12_dW12_0 = dh12_dW12_0 * X[0,0]\n",
    "\n",
    "#dL/dW12_0 = dL/dh12 * dh12/dW12_0\n",
    "dL_dW12_0 = dL_dh12 * dh12_dW12_0\n",
    "\n",
    "# y2 = x1 * W12_0 + x2 * W12_1 +b12\n",
    "# dh12/dW12_1 = dh12/dy2 * dy2/dW12_1\n",
    "# dh12/dy2 = 0 if y2 < 0 and 1 if y2 >= 0 derivative of relu\n",
    "# dy2/dW12_1 = x2\n",
    "dh12_dW12_1 = 0.\n",
    "if y2 >= 0 :\n",
    "    dh12_dW12_1 = 1.\n",
    "dh12_dW12_1 = dh12_dW12_1 * X[0,1]\n",
    "\n",
    "#dL/dW12_1= dL/dh12 * dh12/dW11_1 \n",
    "dL_dW12_1 = dL_dh12 * dh12_dW12_1\n",
    "\n",
    "# y1 = x1 * W11_0 + x2 * W11_1 +b11\n",
    "# dh11/dB11 = dh11/dy1 * dy1/dB11\n",
    "# dh11/dy1 = 0 if y1 < 0 and 1 if y1>=  0 derivative of relu\n",
    "# dy1/dB11 = 1.\n",
    "dh11_dB11 = 0.\n",
    "if y1 >= 0 :\n",
    "    dh11_dB11 = 1.\n",
    "dh11_dB11 = dh11_dB11 * 1.\n",
    "\n",
    "# y2 = x1 * W12_0 + x2 * W12_1 +b12\n",
    "# dh12/dB12 = dh12/dy2 * dy2/dB12\n",
    "# dh12/dy2 = 0 if y2 < 0 and 1 if y2 >= 0 derivative of relu\n",
    "# dy2/dB12 = 1.\n",
    "dh12_dB12 = 0.\n",
    "if y2 >= 0 :\n",
    "    dh12_dB12 = 1.\n",
    "dh12_dB12 = dh12_dB12 * 1.\n",
    "\n",
    "#dL/dB11 = dL/dh11 * dh11/dB11 \n",
    "dL_dB11 = dL_dh11 * dh11_dB11 \n",
    "\n",
    "#dL/dB12 = dL/dh12 * dh12/dB12 + dL/dh12 * dh12/dB12\n",
    "dL_dB12= dL_dh12 * dh12_dB12\n",
    "\n",
    "print('gradients for dense layer kernel:\\n [[', dL_dW11_0 , dL_dW12_0, '],[',dL_dW11_1 , dL_dW12_1,']]')\n",
    "print('gradients for dense_layer bias:\\n [', dL_dB11,dL_dB12, ']')\n",
    "\n",
    "print('gradients for gate_layer kernel:\\n [[', dL_dW21_0 , dL_dW22_0, '],[',dL_dW21_1 , dL_dW22_1,']]')\n",
    "print('gradients for gate_layer bias:\\n [', dL_dB21,dL_dB22, ']')\n",
    "\n",
    "gate_gradients = {'dense layer kernel':[[ dL_dW11_0 , dL_dW12_0],[dL_dW11_1 , dL_dW12_1]],\n",
    "                  'dense layer bias':[dL_dB11,dL_dB12]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross entropy loss\n",
    "actual = y_onehot[:1][0]\n",
    "pred = sel_out\n",
    "print('actual:', actual)\n",
    "print('pred:', pred)\n",
    "print(np.subtract(actual,pred))\n",
    "# loss = np.mean(np.square(np.subtract(actual,pred)))\n",
    "loss = 0.0\n",
    "for i in range(actual.shape[0]):\n",
    "    loss += -1 * (actual[i]*np.log(pred[i]))\n",
    "print('loss:', loss)\n",
    "print('loss:',categorical_crossentropy(actual, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "x: [[0.192768693 0.807231307]\n",
      " [0.768282533 0.231717497]\n",
      " [0.351032 0.648968]\n",
      " ...\n",
      " [0.586429536 0.413570464]\n",
      " [0.626053393 0.373946697]\n",
      " [0.586429536 0.413570464]]\n",
      "[8 2]\n",
      "p: [[0.777836144 0.222163916]\n",
      " [0.303484738 0.696515322]\n",
      " [0.489089429 0.51091063]\n",
      " [0.777836144 0.222163916]] \n",
      "\n",
      "[4 2 2]\n",
      "[[[0.192768693 0.807231307]\n",
      "  [0.768282533 0.231717497]]\n",
      "\n",
      " [[0.351032 0.648968]\n",
      "  [0.564717 0.435282946]]\n",
      "\n",
      " [[0.586429536 0.413570464]\n",
      "  [0.586429536 0.413570464]]\n",
      "\n",
      " [[0.626053393 0.373946697]\n",
      "  [0.586429536 0.413570464]]]\n",
      "o: [[0.320627093 0.679372966]\n",
      " [0.499866873 0.500133097]\n",
      " [0.586429596 0.413570464]\n",
      " [0.617250443 0.382749707]]\n",
      "x: [[0.192768693 0.807231307]\n",
      " [0.768282533 0.231717497]\n",
      " [0.351032 0.648968]\n",
      " ...\n",
      " [0.586429536 0.413570464]\n",
      " [0.626053393 0.373946697]\n",
      " [0.586429536 0.413570464]]\n",
      "[8 2]\n",
      "p: [[0.777836144 0.222163916]\n",
      " [0.303484738 0.696515322]\n",
      " [0.489089429 0.51091063]\n",
      " [0.777836144 0.222163916]] \n",
      "\n",
      "[4 2 2]\n",
      "[[[0.192768693 0.807231307]\n",
      "  [0.768282533 0.231717497]]\n",
      "\n",
      " [[0.351032 0.648968]\n",
      "  [0.564717 0.435282946]]\n",
      "\n",
      " [[0.586429536 0.413570464]\n",
      "  [0.586429536 0.413570464]]\n",
      "\n",
      " [[0.626053393 0.373946697]\n",
      "  [0.586429536 0.413570464]]]\n",
      "o: [[0.320627093 0.679372966]\n",
      " [0.499866873 0.500133097]\n",
      " [0.586429596 0.413570464]\n",
      " [0.617250443 0.382749707]]\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8311 - accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "x: [[0.194072619 0.805927396]\n",
      " [0.352598816 0.647401214]\n",
      " [0.766136587 0.233863384]\n",
      " ...\n",
      " [0.625625134 0.374374926]\n",
      " [0.586183369 0.413816571]\n",
      " [0.586183369 0.413816571]]\n",
      "[8 2]\n",
      "p: [[0.777630627 0.222369418]\n",
      " [0.489068985 0.510931]\n",
      " [0.303640872 0.696359098]\n",
      " [0.777630627 0.222369418]] \n",
      "\n",
      "[4 2 2]\n",
      "[[[0.194072619 0.805927396]\n",
      "  [0.352598816 0.647401214]]\n",
      "\n",
      " [[0.766136587 0.233863384]\n",
      "  [0.561603546 0.438396484]]\n",
      "\n",
      " [[0.586183369 0.413816571]\n",
      "  [0.625625134 0.374374926]]\n",
      "\n",
      " [[0.586183369 0.413816571]\n",
      "  [0.586183369 0.413816571]]]\n",
      "o: [[0.229324013 0.770676]\n",
      " [0.661634326 0.338365704]\n",
      " [0.613649 0.386351019]\n",
      " [0.586183429 0.413816601]]\n",
      "x: [[0.194072619 0.805927396]\n",
      " [0.352598816 0.647401214]\n",
      " [0.766136587 0.233863384]\n",
      " ...\n",
      " [0.625625134 0.374374926]\n",
      " [0.586183369 0.413816571]\n",
      " [0.586183369 0.413816571]]\n",
      "[8 2]\n",
      "p: [[0.777630627 0.222369418]\n",
      " [0.489068985 0.510931]\n",
      " [0.303640872 0.696359098]\n",
      " [0.777630627 0.222369418]] \n",
      "\n",
      "[4 2 2]\n",
      "[[[0.194072619 0.805927396]\n",
      "  [0.352598816 0.647401214]]\n",
      "\n",
      " [[0.766136587 0.233863384]\n",
      "  [0.561603546 0.438396484]]\n",
      "\n",
      " [[0.586183369 0.413816571]\n",
      "  [0.625625134 0.374374926]]\n",
      "\n",
      " [[0.586183369 0.413816571]\n",
      "  [0.586183369 0.413816571]]]\n",
      "o: [[0.229324013 0.770676]\n",
      " [0.661634326 0.338365704]\n",
      " [0.613649 0.386351019]\n",
      " [0.586183429 0.413816601]]\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9298 - accuracy: 0.2500\n",
      "Epoch 3/5\n",
      "x: [[0.362054825 0.637945175]\n",
      " [0.772176206 0.227823824]\n",
      " [0.202332079 0.797668]\n",
      " ...\n",
      " [0.584728479 0.41527158]\n",
      " [0.584728479 0.41527158]\n",
      " [0.584728479 0.41527158]]\n",
      "[8 2]\n",
      "p: [[0.489367276 0.510632634]\n",
      " [0.303916425 0.696083546]\n",
      " [0.777612686 0.222387299]\n",
      " [0.777612686 0.222387299]] \n",
      "\n",
      "[4 2 2]\n",
      "[[[0.362054825 0.637945175]\n",
      "  [0.772176206 0.227823824]]\n",
      "\n",
      " [[0.202332079 0.797668]\n",
      "  [0.573207378 0.426792622]]\n",
      "\n",
      " [[0.624051273 0.375948757]\n",
      "  [0.584728479 0.41527158]]\n",
      "\n",
      " [[0.584728479 0.41527158]\n",
      "  [0.584728479 0.41527158]]]\n",
      "o: [[0.571476161 0.428523779]\n",
      " [0.460492253 0.539507747]\n",
      " [0.615306377 0.384693623]\n",
      " [0.584728479 0.41527158]]\n",
      "x: [[0.362054825 0.637945175]\n",
      " [0.772176206 0.227823824]\n",
      " [0.202332079 0.797668]\n",
      " ...\n",
      " [0.584728479 0.41527158]\n",
      " [0.584728479 0.41527158]\n",
      " [0.584728479 0.41527158]]\n",
      "[8 2]\n",
      "p: [[0.489367276 0.510632634]\n",
      " [0.303916425 0.696083546]\n",
      " [0.777612686 0.222387299]\n",
      " [0.777612686 0.222387299]] \n",
      "\n",
      "[4 2 2]\n",
      "[[[0.362054825 0.637945175]\n",
      "  [0.772176206 0.227823824]]\n",
      "\n",
      " [[0.202332079 0.797668]\n",
      "  [0.573207378 0.426792622]]\n",
      "\n",
      " [[0.624051273 0.375948757]\n",
      "  [0.584728479 0.41527158]]\n",
      "\n",
      " [[0.584728479 0.41527158]\n",
      "  [0.584728479 0.41527158]]]\n",
      "o: [[0.571476161 0.428523779]\n",
      " [0.460492253 0.539507747]\n",
      " [0.615306377 0.384693623]\n",
      " [0.584728479 0.41527158]]\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6353 - accuracy: 0.7500\n",
      "Epoch 4/5\n",
      "x: [[0.364549488 0.635450542]\n",
      " [0.201025426 0.798974574]\n",
      " [0.567929327 0.432070673]\n",
      " ...\n",
      " [0.584485054 0.415515]\n",
      " [0.584485054 0.415515]\n",
      " [0.584485054 0.415515]]\n",
      "[8 2]\n",
      "p: [[0.489184767 0.510815263]\n",
      " [0.777592957 0.222407058]\n",
      " [0.777592957 0.222407058]\n",
      " [0.304083586 0.695916355]] \n",
      "\n",
      "[4 2 2]\n",
      "[[[0.364549488 0.635450542]\n",
      "  [0.201025426 0.798974574]]\n",
      "\n",
      " [[0.567929327 0.432070673]\n",
      "  [0.772008181 0.227991804]]\n",
      "\n",
      " [[0.623934269 0.376065701]\n",
      "  [0.584485054 0.415515]]\n",
      "\n",
      " [[0.584485054 0.415515]\n",
      "  [0.584485054 0.415515]]]\n",
      "o: [[0.281018913 0.718981147]\n",
      " [0.613317907 0.386682093]\n",
      " [0.615160525 0.384839535]\n",
      " [0.584485 0.415514976]]\n",
      "x: [[0.364549488 0.635450542]\n",
      " [0.201025426 0.798974574]\n",
      " [0.567929327 0.432070673]\n",
      " ...\n",
      " [0.584485054 0.415515]\n",
      " [0.584485054 0.415515]\n",
      " [0.584485054 0.415515]]\n",
      "[8 2]\n",
      "p: [[0.489184767 0.510815263]\n",
      " [0.777592957 0.222407058]\n",
      " [0.777592957 0.222407058]\n",
      " [0.304083586 0.695916355]] \n",
      "\n",
      "[4 2 2]\n",
      "[[[0.364549488 0.635450542]\n",
      "  [0.201025426 0.798974574]]\n",
      "\n",
      " [[0.567929327 0.432070673]\n",
      "  [0.772008181 0.227991804]]\n",
      "\n",
      " [[0.623934269 0.376065701]\n",
      "  [0.584485054 0.415515]]\n",
      "\n",
      " [[0.584485054 0.415515]\n",
      "  [0.584485054 0.415515]]]\n",
      "o: [[0.281018913 0.718981147]\n",
      " [0.613317907 0.386682093]\n",
      " [0.615160525 0.384839535]\n",
      " [0.584485 0.415514976]]\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8978 - accuracy: 0.2500\n",
      "Epoch 5/5\n",
      "x: [[0.778907657 0.221092373]\n",
      " [0.374299556 0.625700414]\n",
      " [0.579585969 0.420414031]\n",
      " ...\n",
      " [0.622337162 0.377662867]\n",
      " [0.583028376 0.416971564]\n",
      " [0.583028376 0.416971564]]\n",
      "[8 2]\n",
      "p: [[0.304534614 0.695465446]\n",
      " [0.489757925 0.510242105]\n",
      " [0.777653635 0.222346395]\n",
      " [0.777653635 0.222346395]] \n",
      "\n",
      "[4 2 2]\n",
      "[[[0.778907657 0.221092373]\n",
      "  [0.374299556 0.625700414]]\n",
      "\n",
      " [[0.579585969 0.420414031]\n",
      "  [0.208654627 0.791345417]]\n",
      "\n",
      " [[0.583028376 0.416971564]\n",
      "  [0.622337162 0.377662867]]\n",
      "\n",
      " [[0.583028376 0.416971564]\n",
      "  [0.583028376 0.416971564]]]\n",
      "o: [[0.497516751 0.502483308]\n",
      " [0.390321195 0.609678864]\n",
      " [0.591768563 0.408231437]\n",
      " [0.583028436 0.416971594]]\n",
      "x: [[0.778907657 0.221092373]\n",
      " [0.374299556 0.625700414]\n",
      " [0.579585969 0.420414031]\n",
      " ...\n",
      " [0.622337162 0.377662867]\n",
      " [0.583028376 0.416971564]\n",
      " [0.583028376 0.416971564]]\n",
      "[8 2]\n",
      "p: [[0.304534614 0.695465446]\n",
      " [0.489757925 0.510242105]\n",
      " [0.777653635 0.222346395]\n",
      " [0.777653635 0.222346395]] \n",
      "\n",
      "[4 2 2]\n",
      "[[[0.778907657 0.221092373]\n",
      "  [0.374299556 0.625700414]]\n",
      "\n",
      " [[0.579585969 0.420414031]\n",
      "  [0.208654627 0.791345417]]\n",
      "\n",
      " [[0.583028376 0.416971564]\n",
      "  [0.622337162 0.377662867]]\n",
      "\n",
      " [[0.583028376 0.416971564]\n",
      "  [0.583028376 0.416971564]]]\n",
      "o: [[0.497516751 0.502483308]\n",
      " [0.390321195 0.609678864]\n",
      " [0.591768563 0.408231437]\n",
      " [0.583028436 0.416971594]]\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7661 - accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14287c250>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector_model.fit(X, y_onehot, epochs=5, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients for expert_1 kernel:\n",
    " [[ -0.14871660664237052 0.1487166066423705 ],[ -0.1707620017944827 0.17076200179448267 ]]\n",
    "gradients for exp_layer bias:\n",
    " [ -0.12335146791948583 0.1233514679194858 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(expert_1_output)\n",
    "print(expert_2_output)\n",
    "print(gate_output)\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_latest)",
   "language": "python",
   "name": "tf_latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
