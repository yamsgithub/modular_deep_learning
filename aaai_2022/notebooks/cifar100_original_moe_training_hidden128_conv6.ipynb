{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with CIFAR10 Dataset and Original MoE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiments in this notebook include training the original MoE models as follows:\n",
    "\n",
    "1. original MoE without regularization.\n",
    "2. original MoE with $L_{importance}$ regularization.\n",
    "3. original MoE with $L_s$ regularization.\n",
    "4. train a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import matplotlib.cm as cm  #Â colormaps\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "from math import ceil, sin, cos, radians\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18, resnet50, resnet101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print('device', device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('device', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import MoE expectation model. All experiments for this dataset are done with the expectation model as it\n",
    "# provides the best guarantee of interpretable task decompositions\n",
    "from moe_models.moe_expectation_model import moe_expectation_model\n",
    "from helper.moe_models import cross_entropy_loss\n",
    "from helper.visualise_results import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: Pre-trained models are provided to check the results of all the experiments if you do not have the time to train all the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to where the trained models and figures will be stored. You can change this as you see fit.\n",
    "fig_path = '../figures/hidden64'\n",
    "model_path = '../models/hidden64'\n",
    "pre_trained_model_path = '../models/pre_trained'\n",
    "results_path = '../results'\n",
    "\n",
    "if not os.path.exists(fig_path):\n",
    "    os.mkdir(fig_path)\n",
    "if not os.path.exists(model_path):\n",
    "    os.mkdir(model_path)\n",
    "if not os.path.exists(results_path):\n",
    "    os.mkdir(results_path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = ((0.5074,0.4867,0.4411),(0.2011,0.1987,0.2025))\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32,padding=4,padding_mode=\"reflect\"),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(*stats)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(*stats)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset CIFAR100\n",
       "     Number of datapoints: 10000\n",
       "     Root location: ./data\n",
       "     Split: Test\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "                Normalize(mean=(0.5074, 0.4867, 0.4411), std=(0.2011, 0.1987, 0.2025))\n",
       "            ),\n",
       " Dataset CIFAR100\n",
       "     Number of datapoints: 50000\n",
       "     Root location: ./data\n",
       "     Split: Train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                RandomHorizontalFlip(p=0.5)\n",
       "                RandomCrop(size=(32, 32), padding=4)\n",
       "                ToTensor()\n",
       "                Normalize(mean=(0.5074, 0.4867, 0.4411), std=(0.2011, 0.1987, 0.2025))\n",
       "            ))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar100_trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
    "cifar100_testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)\n",
    "cifar100_testset, cifar100_trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainsize = 50000\n",
    "testsize = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar100_trainloader = torch.utils.data.DataLoader(torch.utils.data.Subset(cifar100_trainset, range(trainsize)), batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2, pin_memory=True)\n",
    "cifar100_testloader = torch.utils.data.DataLoader(torch.utils.data.Subset(cifar100_testset, range(testsize)), batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple',\n",
       " 'aquarium_fish',\n",
       " 'baby',\n",
       " 'bear',\n",
       " 'beaver',\n",
       " 'bed',\n",
       " 'bee',\n",
       " 'beetle',\n",
       " 'bicycle',\n",
       " 'bottle',\n",
       " 'bowl',\n",
       " 'boy',\n",
       " 'bridge',\n",
       " 'bus',\n",
       " 'butterfly',\n",
       " 'camel',\n",
       " 'can',\n",
       " 'castle',\n",
       " 'caterpillar',\n",
       " 'cattle',\n",
       " 'chair',\n",
       " 'chimpanzee',\n",
       " 'clock',\n",
       " 'cloud',\n",
       " 'cockroach',\n",
       " 'couch',\n",
       " 'cra',\n",
       " 'crocodile',\n",
       " 'cup',\n",
       " 'dinosaur',\n",
       " 'dolphin',\n",
       " 'elephant',\n",
       " 'flatfish',\n",
       " 'forest',\n",
       " 'fox',\n",
       " 'girl',\n",
       " 'hamster',\n",
       " 'house',\n",
       " 'kangaroo',\n",
       " 'keyboard',\n",
       " 'lamp',\n",
       " 'lawn_mower',\n",
       " 'leopard',\n",
       " 'lion',\n",
       " 'lizard',\n",
       " 'lobster',\n",
       " 'man',\n",
       " 'maple_tree',\n",
       " 'motorcycle',\n",
       " 'mountain',\n",
       " 'mouse',\n",
       " 'mushroom',\n",
       " 'oak_tree',\n",
       " 'orange',\n",
       " 'orchid',\n",
       " 'otter',\n",
       " 'palm_tree',\n",
       " 'pear',\n",
       " 'pickup_truck',\n",
       " 'pine_tree',\n",
       " 'plain',\n",
       " 'plate',\n",
       " 'poppy',\n",
       " 'porcupine',\n",
       " 'possum',\n",
       " 'rabbit',\n",
       " 'raccoon',\n",
       " 'ray',\n",
       " 'road',\n",
       " 'rocket',\n",
       " 'rose',\n",
       " 'sea',\n",
       " 'seal',\n",
       " 'shark',\n",
       " 'shrew',\n",
       " 'skunk',\n",
       " 'skyscraper',\n",
       " 'snail',\n",
       " 'snake',\n",
       " 'spider',\n",
       " 'squirrel',\n",
       " 'streetcar',\n",
       " 'sunflower',\n",
       " 'sweet_pepper',\n",
       " 'table',\n",
       " 'tank',\n",
       " 'telephone',\n",
       " 'television',\n",
       " 'tiger',\n",
       " 'tractor',\n",
       " 'train',\n",
       " 'trout',\n",
       " 'tulip',\n",
       " 'turtle',\n",
       " 'wardrobe',\n",
       " 'whale',\n",
       " 'willow_tree',\n",
       " 'wolf',\n",
       " 'woman',\n",
       " 'worm']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "with open('data/cifar100_class_names.txt','r') as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=' ')\n",
    "    classes_cifar100 = []\n",
    "    for row in csvreader:\n",
    "        if row:\n",
    "            classes_cifar100.append(row[1])\n",
    "\n",
    "classes_cifar100            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to display the images\n",
    "def plot_colour_images(images_to_plot, titles=None, nrows=None, ncols=6, thefigsize=(18,18)):\n",
    "    # images_to_plot: list of images to be displayed\n",
    "    # titles: list of titles corresponding to the images\n",
    "    # ncols: The number of images per row to display. The number of rows \n",
    "    #        is computed from the number of images to display and the ncols\n",
    "    # theFigsize: The size of the layour of all the displayed images\n",
    "    \n",
    "    n_images = images_to_plot.shape[0]\n",
    "    \n",
    "    # Compute the number of rows\n",
    "    if nrows is None:\n",
    "        nrows = np.ceil(n_images/ncols).astype(int)\n",
    "    \n",
    "    fig,ax = plt.subplots(nrows, ncols, sharex=True, sharey=True, figsize=thefigsize)\n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    for i in range(n_images):\n",
    "        img = images_to_plot[i,:,:,:]\n",
    "        npimg = np.clip(img.numpy(),0,1)\n",
    "        ax[i].imshow(npimg)\n",
    "        ax[i].axis('off')  \n",
    "        if titles is not None and i<10:\n",
    "            ax[i].set_title(titles[i%10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get some random training images\n",
    "# dataiter = iter(cifar100_trainloader)\n",
    "# images, labels = dataiter.next()\n",
    "\n",
    "# images_to_plot = []\n",
    "# count = 0\n",
    "# selected_labels = []\n",
    "# for i in range(100):\n",
    "#     if count == 10:\n",
    "#         break\n",
    "#     index = np.where(labels==i)[0]\n",
    "#     if len(index) >= 3:\n",
    "#         selected_labels.append(i)\n",
    "#         images_to_plot.append(images[index[0:3],:,:])\n",
    "#         count += 1\n",
    "    \n",
    "# selected_labels = [classes_cifar100[i] for i in selected_labels]\n",
    "# images_to_plot = torch.transpose(torch.stack(images_to_plot),0,1)\n",
    "# new_shape = images_to_plot.shape\n",
    "# images_to_plot = images_to_plot.reshape(new_shape[0]*new_shape[1], new_shape[2], new_shape[3], new_shape[4])\n",
    "# images_to_plot = images_to_plot.permute(0,2,3,1)\n",
    "# plot_colour_images(images_to_plot, nrows=3, ncols=10,thefigsize=(20,6), titles=selected_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define expert and gate networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional network with one convultional layer and 2 hidden layers with ReLU activation\n",
    "class expert_layers(nn.Module):\n",
    "    def __init__(self, num_classes, channels=3):\n",
    "        super(expert_layers, self).__init__()\n",
    "        filter_size = 3\n",
    "        self.filters = 8\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=self.filters, kernel_size=filter_size, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=self.filters, out_channels=self.filters*2, kernel_size=filter_size, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(self.filters*2)\n",
    "        self.mp = nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels= self.filters*2, out_channels=self.filters*4, kernel_size=filter_size, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=self.filters*4, out_channels=self.filters*4, kernel_size=filter_size, stride=1, padding=1,bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(self.filters*4)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(in_channels= self.filters*4, out_channels=self.filters*8, kernel_size=filter_size, stride=1, padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=self.filters*8, out_channels=self.filters*8, kernel_size=filter_size, stride=1, padding=1,bias=False)\n",
    "        self.bn8 = nn.BatchNorm2d(self.filters*8)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.filters*8*2*2,512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        \n",
    "        self.out = nn.Linear(in_features=128, out_features=num_classes)\n",
    "                        \n",
    "    def forward(self, t):\n",
    "        # conv 1        \n",
    "        x = F.relu(self.conv1(t))\n",
    "        x = self.mp(F.relu(self.conv2(x)))\n",
    "                \n",
    "        x = self.mp(F.relu(self.bn4(self.conv3(x))))\n",
    "        \n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.mp(F.relu(self.conv5(x)))\n",
    "        \n",
    "        x = self.mp(F.relu(self.bn8(self.conv6(x))))\n",
    "        \n",
    "        \n",
    "                    \n",
    "        x = x.reshape(-1, self.filters*8*2*2)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        x = self.out(x)\n",
    "        \n",
    "        # print('E', x.shape)\n",
    "        \n",
    "        # output\n",
    "        x = F.softmax(x, dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional network with one convultional layer and 2 hidden layers with ReLU activation\n",
    "class gate_layers(nn.Module):\n",
    "    def __init__(self, num_experts):\n",
    "        super(gate_layers, self).__init__()\n",
    "        # define layers\n",
    "        filter_size = 3\n",
    "        self.filters = 64\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=self.filters, kernel_size=filter_size, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=self.filters, out_channels=self.filters*2, kernel_size=filter_size, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(self.filters*2)\n",
    "        self.mp = nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels= self.filters*2, out_channels=self.filters*4, kernel_size=filter_size, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=self.filters*4, out_channels=self.filters*4, kernel_size=filter_size, stride=1, padding=1,bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(self.filters*4)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(in_channels= self.filters*4, out_channels=self.filters*8, kernel_size=filter_size, stride=1, padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=self.filters*8, out_channels=self.filters*8, kernel_size=filter_size, stride=1, padding=1,bias=False)\n",
    "        self.bn8 = nn.BatchNorm2d(self.filters*8)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.filters*8*2*2,1024)\n",
    "        self.fc2 = nn.Linear(1024, 128)\n",
    "        \n",
    "        self.out = nn.Linear(in_features=128, out_features=num_experts)\n",
    "                        \n",
    "    def forward(self,t , T=1.0, y=None):\n",
    "        # conv 1        \n",
    "        x = F.relu(self.conv1(t))\n",
    "        x = self.mp(F.relu(self.conv2(x)))\n",
    "                \n",
    "        x = self.mp(F.relu(self.bn4(self.conv3(x))))\n",
    "        \n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.mp(F.relu(self.conv5(x)))\n",
    "        \n",
    "        x = self.mp(F.relu(self.bn8(self.conv6(x))))\n",
    "        \n",
    "        \n",
    "                    \n",
    "        x = x.reshape(-1, self.filters*8*2*2)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        x = self.out(x)\n",
    "        \n",
    "        # print('G', x.shape)\n",
    "        \n",
    "        # output\n",
    "        x = F.softmax(x/T, dim=1)        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a set of experts\n",
    "def experts(num_experts, num_classes, expert_layers_type=expert_layers):\n",
    "    models = []\n",
    "    for i in range(num_experts):\n",
    "        models.append(expert_layers_type(num_classes))\n",
    "    return nn.ModuleList(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convolutional network with one convultional layer and 2 hidden layers with ReLU activation\n",
    "class single_model(nn.Module):\n",
    "    def __init__(self, num_classes, channels=3):\n",
    "        super(single_model, self).__init__()\n",
    "        filter_size = 3\n",
    "        self.filters = 32\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=self.filters, kernel_size=filter_size, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=self.filters, out_channels=self.filters*2, kernel_size=filter_size, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(self.filters*2)\n",
    "        self.mp = nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels= self.filters*2, out_channels=self.filters*4, kernel_size=filter_size, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=self.filters*4, out_channels=self.filters*4, kernel_size=filter_size, stride=1, padding=1,bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(self.filters*4)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(in_channels= self.filters*4, out_channels=self.filters*8, kernel_size=filter_size, stride=1, padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=self.filters*8, out_channels=self.filters*8, kernel_size=filter_size, stride=1, padding=1,bias=False)\n",
    "        self.bn8 = nn.BatchNorm2d(self.filters*8)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.filters*8*2*2,512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        \n",
    "        self.out = nn.Linear(in_features=128, out_features=num_classes)\n",
    "                        \n",
    "    def forward(self, t):\n",
    "        # conv 1        \n",
    "        x = F.relu(self.conv1(t))\n",
    "        x = self.mp(F.relu(self.conv2(x)))\n",
    "                \n",
    "        x = self.mp(F.relu(self.bn4(self.conv3(x))))\n",
    "        \n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.mp(F.relu(self.conv5(x)))\n",
    "        \n",
    "        x = self.mp(F.relu(self.bn8(self.conv6(x))))\n",
    "        \n",
    "        # print(x.shape)\n",
    "                    \n",
    "        x = x.reshape(-1, self.filters*8*2*2)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        x = self.out(x)\n",
    "        \n",
    "        # output\n",
    "        x = F.softmax(x, dim=1)\n",
    "                \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize configurations and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy of the model\n",
    "def accuracy(out, yb, mean=True):\n",
    "    preds = torch.argmax(out, dim=1).to(device, non_blocking=True)\n",
    "    if mean:\n",
    "        return (preds == yb).float().mean()\n",
    "    else:\n",
    "        return (preds == yb).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to train models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to train original model with and without regularization\n",
    "\n",
    "* w_importance_range is the range of values for the $w_{importance}$ hyperparameter of the $L_{importance}$ regularization.\n",
    "* w_sample_sim_same_range is the range of values for $\\beta_s$ hyperparameter of the $L_s$ regularization.\n",
    "* w_sample_sim_diff_range is the range of values for $\\beta_d$ hyperparameter of the $L_s$ regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def train_original_model(model_1, trainloader, testloader, runs, temps=[[1.0]*20], \n",
    "                         w_importance_range=[0.0], w_sample_sim_same_range=[0.0], \n",
    "                         w_sample_sim_diff_range=[0.0],\n",
    "                         num_classes=10, total_experts=5, num_epochs=20):\n",
    "\n",
    "    for T, w_importance, w_sample_sim_same, w_sample_sim_diff in product(temps, w_importance_range, \n",
    "                                                                         w_sample_sim_same_range,  w_sample_sim_diff_range):\n",
    "        \n",
    "        print('w_importance','{:.1f}'.format(w_importance))\n",
    "        if w_sample_sim_same < 1:\n",
    "            print('w_sample_sim_same',str(w_sample_sim_same))\n",
    "        else:\n",
    "            print('w_sample_sim_same','{:.1f}'.format(w_sample_sim_same))\n",
    "        \n",
    "        if w_sample_sim_diff < 1:\n",
    "            print('w_sample_sim_diff',str(w_sample_sim_diff))\n",
    "        else:\n",
    "            print('w_sample_sim_diff','{:.1f}'.format(w_sample_sim_diff))\n",
    "\n",
    "        \n",
    "        for run in range(1, runs+1):\n",
    "            \n",
    "            print('Run:', run)\n",
    "            \n",
    "            n_run_models_1 = []\n",
    "            \n",
    "            models = {'moe_expectation_model':{'model':moe_expectation_model,'loss':cross_entropy_loss().to(device),\n",
    "                                               'experts':{}},}\n",
    "            for key, val in models.items():\n",
    "\n",
    "                expert_models = experts(total_experts, num_classes).to(device)\n",
    "\n",
    "                gate_model = gate_layers(total_experts).to(device)\n",
    "\n",
    "                moe_model = val['model'](total_experts, num_classes,\n",
    "                                         experts=expert_models, gate=gate_model,device=device).to(device)\n",
    "                \n",
    "                optimizer_moe = optim.Adam(moe_model.parameters(), lr=0.001, amsgrad=False, weight_decay=1e-3)\n",
    "                \n",
    "               \n",
    "                hist = moe_model.train(trainloader, testloader,  val['loss'], optimizer_moe = optimizer_moe,\n",
    "                                       T = T, w_importance=w_importance, w_sample_sim_same = w_sample_sim_same, \n",
    "                                       w_sample_sim_diff = w_sample_sim_diff, \n",
    "                                       accuracy=accuracy, epochs=num_epochs)\n",
    "                val['experts'][total_experts] = {'model':moe_model, 'history':hist}                \n",
    "\n",
    "\n",
    "            # Save all the trained models\n",
    "            plot_file = generate_plot_file(model_1, T[0], w_importance=w_importance, w_sample_sim_same=w_sample_sim_same,w_sample_sim_diff=w_sample_sim_diff,\n",
    "                                           specific=str(num_classes)+'_'+str(total_experts)+'_models.pt')\n",
    "            \n",
    "            if os.path.exists(os.path.join(model_path, plot_file)):\n",
    "                n_run_models_1 = torch.load(open(os.path.join(model_path, plot_file),'rb'))\n",
    "            n_run_models_1.append(models)\n",
    "            torch.save(n_run_models_1,open(os.path.join(model_path, plot_file),'wb'))\n",
    "            n_run_models_1 = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to train the single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_single_model(model_name, trainloader, testloader, num_classes, num_epochs, runs):\n",
    "    \n",
    "    loss_criterion = cross_entropy_loss()\n",
    "    \n",
    "    n_runs = {'models':[], 'history':[]}\n",
    "    \n",
    "    for run in range(1, runs+1):\n",
    "        \n",
    "        print('Run', run)\n",
    "        \n",
    "        model = single_model(num_classes).to(device)\n",
    "        history = {'loss':[], 'accuracy':[], 'val_accuracy':[]}\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001, amsgrad=False, weight_decay=1e-3)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0.0\n",
    "            train_running_accuracy = 0.0\n",
    "            num_batches = 0\n",
    "\n",
    "            for inputs, labels in trainloader:\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss = loss_criterion(outputs, None, None, labels)\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                acc = accuracy(outputs, labels)\n",
    "                train_running_accuracy += acc\n",
    "\n",
    "                num_batches += 1\n",
    "\n",
    "            test_running_accuracy = 0.0\n",
    "            test_num_batches = 0\n",
    "            \n",
    "            for test_inputs, test_labels in testloader:\n",
    "                test_inputs, test_labels = test_inputs.to(device, non_blocking=True), test_labels.to(device, non_blocking=True)\n",
    "                test_outputs = model(test_inputs)              \n",
    "                test_running_accuracy += accuracy(test_outputs, test_labels)\n",
    "                test_num_batches += 1\n",
    "                \n",
    "            loss = (running_loss/num_batches)\n",
    "            train_accuracy = (train_running_accuracy/num_batches)\n",
    "            test_accuracy = (test_running_accuracy/test_num_batches)\n",
    "            \n",
    "            history['loss'].append(loss)\n",
    "            history['accuracy'].append(train_accuracy.item())\n",
    "            history['val_accuracy'].append(test_accuracy.item())\n",
    "            \n",
    "            print('epoch %d' % epoch,\n",
    "                  'training loss %.2f' % loss,\n",
    "                   ', training accuracy %.2f' % train_accuracy,\n",
    "                   ', test accuracy %.2f' % test_accuracy\n",
    "                   )\n",
    "            \n",
    "        plot_file = generate_plot_file(model_name, specific=str(num_classes)+'_models.pt')\n",
    "        if os.path.exists(os.path.join(model_path, plot_file)):\n",
    "            n_runs = torch.load(open(os.path.join(model_path, plot_file),'rb'))\n",
    "        n_runs['models'].append(model)\n",
    "        n_runs['history'].append(history)        \n",
    "        torch.save(n_runs, open(os.path.join(model_path, plot_file),'wb'))\n",
    "        \n",
    "        n_runs = {'models':[], 'history':[]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Experiment 1: Original MoE model trained without gate regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with gate and expert parameters initialized to default values\n",
    "model_1 = 'cifar100_without_reg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_experts = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = [[1.0]*num_epochs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_importance 0.0\n",
      "w_sample_sim_same 0.0\n",
      "w_sample_sim_diff 0.0\n",
      "Run: 1\n",
      "epoch 0 training loss 4.05 , training accuracy 0.09 , test accuracy 0.10\n",
      "epoch 1 training loss 3.68 , training accuracy 0.14 , test accuracy 0.15\n",
      "epoch 2 training loss 3.45 , training accuracy 0.18 , test accuracy 0.19\n",
      "epoch 3 training loss 3.25 , training accuracy 0.22 , test accuracy 0.22\n",
      "epoch 4 training loss 3.06 , training accuracy 0.25 , test accuracy 0.25\n",
      "epoch 5 training loss 2.87 , training accuracy 0.29 , test accuracy 0.29\n",
      "epoch 6 training loss 2.72 , training accuracy 0.33 , test accuracy 0.31\n",
      "epoch 7 training loss 2.58 , training accuracy 0.36 , test accuracy 0.33\n",
      "epoch 8 training loss 2.45 , training accuracy 0.38 , test accuracy 0.38\n",
      "epoch 9 training loss 2.35 , training accuracy 0.41 , test accuracy 0.39\n",
      "epoch 10 training loss 2.26 , training accuracy 0.43 , test accuracy 0.40\n",
      "epoch 11 training loss 2.18 , training accuracy 0.44 , test accuracy 0.42\n",
      "epoch 12 training loss 2.10 , training accuracy 0.46 , test accuracy 0.43\n",
      "epoch 13 training loss 2.04 , training accuracy 0.48 , test accuracy 0.45\n",
      "epoch 14 training loss 1.98 , training accuracy 0.49 , test accuracy 0.47\n",
      "epoch 15 training loss 1.93 , training accuracy 0.50 , test accuracy 0.46\n",
      "epoch 16 training loss 1.88 , training accuracy 0.52 , test accuracy 0.48\n",
      "epoch 17 training loss 1.84 , training accuracy 0.53 , test accuracy 0.48\n",
      "epoch 18 training loss 1.80 , training accuracy 0.54 , test accuracy 0.49\n",
      "epoch 19 training loss 1.77 , training accuracy 0.54 , test accuracy 0.50\n",
      "epoch 20 training loss 1.58 , training accuracy 0.57 , test accuracy 0.54\n",
      "epoch 21 training loss 1.52 , training accuracy 0.58 , test accuracy 0.54\n",
      "epoch 22 training loss 1.50 , training accuracy 0.59 , test accuracy 0.54\n",
      "epoch 23 training loss 1.47 , training accuracy 0.60 , test accuracy 0.56\n",
      "epoch 24 training loss 1.46 , training accuracy 0.60 , test accuracy 0.54\n",
      "epoch 25 training loss 1.43 , training accuracy 0.61 , test accuracy 0.55\n",
      "epoch 26 training loss 1.42 , training accuracy 0.61 , test accuracy 0.55\n",
      "epoch 27 training loss 1.41 , training accuracy 0.61 , test accuracy 0.56\n",
      "epoch 28 training loss 1.39 , training accuracy 0.62 , test accuracy 0.57\n",
      "epoch 29 training loss 1.38 , training accuracy 0.62 , test accuracy 0.56\n",
      "epoch 30 training loss 1.36 , training accuracy 0.63 , test accuracy 0.57\n",
      "epoch 31 training loss 1.35 , training accuracy 0.63 , test accuracy 0.57\n",
      "epoch 32 training loss 1.34 , training accuracy 0.64 , test accuracy 0.57\n",
      "epoch 33 training loss 1.32 , training accuracy 0.64 , test accuracy 0.57\n",
      "epoch 34 training loss 1.31 , training accuracy 0.64 , test accuracy 0.57\n",
      "epoch 35 training loss 1.30 , training accuracy 0.64 , test accuracy 0.57\n",
      "epoch 36 training loss 1.29 , training accuracy 0.65 , test accuracy 0.58\n",
      "epoch 37 training loss 1.27 , training accuracy 0.65 , test accuracy 0.57\n",
      "epoch 38 training loss 1.27 , training accuracy 0.65 , test accuracy 0.58\n",
      "epoch 39 training loss 1.26 , training accuracy 0.66 , test accuracy 0.58\n",
      "epoch 40 training loss 1.17 , training accuracy 0.67 , test accuracy 0.60\n",
      "epoch 41 training loss 1.15 , training accuracy 0.67 , test accuracy 0.59\n",
      "epoch 42 training loss 1.13 , training accuracy 0.68 , test accuracy 0.59\n",
      "epoch 43 training loss 1.13 , training accuracy 0.68 , test accuracy 0.59\n",
      "epoch 44 training loss 1.12 , training accuracy 0.68 , test accuracy 0.59\n",
      "epoch 45 training loss 1.11 , training accuracy 0.68 , test accuracy 0.59\n",
      "epoch 46 training loss 1.11 , training accuracy 0.68 , test accuracy 0.59\n",
      "epoch 47 training loss 1.10 , training accuracy 0.68 , test accuracy 0.60\n",
      "epoch 48 training loss 1.09 , training accuracy 0.69 , test accuracy 0.60\n",
      "epoch 49 training loss 1.08 , training accuracy 0.69 , test accuracy 0.60\n",
      "epoch 50 training loss 1.08 , training accuracy 0.69 , test accuracy 0.59\n",
      "epoch 51 training loss 1.07 , training accuracy 0.69 , test accuracy 0.60\n",
      "epoch 52 training loss 1.07 , training accuracy 0.70 , test accuracy 0.60\n",
      "epoch 53 training loss 1.07 , training accuracy 0.70 , test accuracy 0.60\n",
      "epoch 54 training loss 1.06 , training accuracy 0.70 , test accuracy 0.60\n",
      "epoch 55 training loss 1.05 , training accuracy 0.70 , test accuracy 0.60\n",
      "epoch 56 training loss 1.05 , training accuracy 0.70 , test accuracy 0.60\n",
      "epoch 57 training loss 1.05 , training accuracy 0.70 , test accuracy 0.60\n",
      "epoch 58 training loss 1.05 , training accuracy 0.70 , test accuracy 0.59\n",
      "epoch 59 training loss 1.04 , training accuracy 0.70 , test accuracy 0.60\n",
      "epoch 60 training loss 1.00 , training accuracy 0.71 , test accuracy 0.61\n",
      "epoch 61 training loss 0.99 , training accuracy 0.71 , test accuracy 0.61\n",
      "epoch 62 training loss 0.99 , training accuracy 0.71 , test accuracy 0.61\n",
      "epoch 63 training loss 0.99 , training accuracy 0.71 , test accuracy 0.60\n",
      "epoch 64 training loss 0.98 , training accuracy 0.71 , test accuracy 0.60\n",
      "epoch 65 training loss 0.98 , training accuracy 0.71 , test accuracy 0.61\n",
      "epoch 66 training loss 0.98 , training accuracy 0.71 , test accuracy 0.61\n",
      "epoch 67 training loss 0.98 , training accuracy 0.71 , test accuracy 0.61\n",
      "epoch 68 training loss 0.97 , training accuracy 0.72 , test accuracy 0.61\n",
      "epoch 69 training loss 0.97 , training accuracy 0.72 , test accuracy 0.61\n",
      "epoch 70 training loss 0.97 , training accuracy 0.72 , test accuracy 0.61\n",
      "epoch 71 training loss 0.96 , training accuracy 0.72 , test accuracy 0.61\n",
      "epoch 72 training loss 0.96 , training accuracy 0.72 , test accuracy 0.60\n",
      "epoch 73 training loss 0.96 , training accuracy 0.72 , test accuracy 0.61\n",
      "epoch 74 training loss 0.95 , training accuracy 0.72 , test accuracy 0.60\n",
      "epoch 75 training loss 0.96 , training accuracy 0.72 , test accuracy 0.61\n",
      "epoch 76 training loss 0.96 , training accuracy 0.72 , test accuracy 0.60\n",
      "epoch 77 training loss 0.95 , training accuracy 0.72 , test accuracy 0.61\n",
      "epoch 78 training loss 0.95 , training accuracy 0.72 , test accuracy 0.60\n",
      "epoch 79 training loss 0.95 , training accuracy 0.72 , test accuracy 0.60\n"
     ]
    }
   ],
   "source": [
    "train_original_model(model_1, cifar100_trainloader, cifar100_testloader, runs, temps=temps,\n",
    "                     num_classes=num_classes, total_experts=total_experts, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Experiment 2: Original MoE model trained with $L_{importance}$ regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with gate and expert parameters initialized to default values\n",
    "model_2 = 'cifar_with_reg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_experts = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w_importance_range = [i * 0.2 for i in range(1, 6)]\n",
    "w_importance_range = [0.6]\n",
    "print('w_importance_range = ', ['{:.1f}'.format(w) for w in w_importance_range])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = [[1]*num_epochs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_original_model(model_2, cifar100_trainloader, cifar100_testloader, runs, temps=temps,\n",
    "                     w_importance_range=w_importance_range, num_classes=num_classes, \n",
    "                     total_experts=total_experts, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Experiment 3: Original MoE model trained with sample similarity regularization, $L_s$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = 'cifar_with_reg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_experts = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = [[1]*num_epochs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_sample_sim_same_range = [1e-4]\n",
    "w_sample_sim_diff_range = [1e-7]\n",
    "print('w_sample_sim_same_range = ', w_sample_sim_same_range)\n",
    "print('w_sample_sim_diff_range = ', w_sample_sim_diff_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_original_model(model_3, cifar100_trainloader, cifar100_testloader, runs, temps=temps,\n",
    "                                     w_sample_sim_same_range=w_sample_sim_same_range, w_sample_sim_diff_range=w_sample_sim_diff_range, \n",
    "                                     num_classes=num_classes, total_experts=total_experts, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Experiment 4: Training the single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = 'cifar100_single_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1\n",
      "epoch 0 training loss 4.01 , training accuracy 0.08 , test accuracy 0.12\n",
      "epoch 1 training loss 3.43 , training accuracy 0.17 , test accuracy 0.20\n",
      "epoch 2 training loss 3.01 , training accuracy 0.25 , test accuracy 0.26\n",
      "epoch 3 training loss 2.69 , training accuracy 0.32 , test accuracy 0.35\n",
      "epoch 4 training loss 2.45 , training accuracy 0.37 , test accuracy 0.38\n",
      "epoch 5 training loss 2.28 , training accuracy 0.41 , test accuracy 0.42\n",
      "epoch 6 training loss 2.12 , training accuracy 0.46 , test accuracy 0.45\n",
      "epoch 7 training loss 2.00 , training accuracy 0.49 , test accuracy 0.46\n",
      "epoch 8 training loss 1.91 , training accuracy 0.50 , test accuracy 0.47\n",
      "epoch 9 training loss 1.84 , training accuracy 0.53 , test accuracy 0.48\n",
      "epoch 10 training loss 1.76 , training accuracy 0.55 , test accuracy 0.52\n",
      "epoch 11 training loss 1.71 , training accuracy 0.56 , test accuracy 0.53\n",
      "epoch 12 training loss 1.65 , training accuracy 0.58 , test accuracy 0.53\n",
      "epoch 13 training loss 1.61 , training accuracy 0.59 , test accuracy 0.54\n",
      "epoch 14 training loss 1.57 , training accuracy 0.60 , test accuracy 0.54\n",
      "epoch 15 training loss 1.54 , training accuracy 0.61 , test accuracy 0.53\n",
      "epoch 16 training loss 1.50 , training accuracy 0.62 , test accuracy 0.54\n",
      "epoch 17 training loss 1.47 , training accuracy 0.63 , test accuracy 0.56\n",
      "epoch 18 training loss 1.44 , training accuracy 0.63 , test accuracy 0.57\n",
      "epoch 19 training loss 1.42 , training accuracy 0.64 , test accuracy 0.56\n",
      "epoch 20 training loss 1.40 , training accuracy 0.64 , test accuracy 0.56\n",
      "epoch 21 training loss 1.37 , training accuracy 0.65 , test accuracy 0.57\n",
      "epoch 22 training loss 1.35 , training accuracy 0.66 , test accuracy 0.58\n",
      "epoch 23 training loss 1.33 , training accuracy 0.66 , test accuracy 0.57\n",
      "epoch 24 training loss 1.31 , training accuracy 0.67 , test accuracy 0.57\n",
      "epoch 25 training loss 1.30 , training accuracy 0.67 , test accuracy 0.59\n",
      "epoch 26 training loss 1.28 , training accuracy 0.68 , test accuracy 0.58\n",
      "epoch 27 training loss 1.26 , training accuracy 0.68 , test accuracy 0.58\n",
      "epoch 28 training loss 1.26 , training accuracy 0.68 , test accuracy 0.59\n",
      "epoch 29 training loss 1.23 , training accuracy 0.69 , test accuracy 0.59\n",
      "epoch 30 training loss 1.22 , training accuracy 0.70 , test accuracy 0.59\n",
      "epoch 31 training loss 1.21 , training accuracy 0.70 , test accuracy 0.60\n",
      "epoch 32 training loss 1.20 , training accuracy 0.70 , test accuracy 0.60\n",
      "epoch 33 training loss 1.20 , training accuracy 0.70 , test accuracy 0.60\n",
      "epoch 34 training loss 1.18 , training accuracy 0.70 , test accuracy 0.60\n",
      "epoch 35 training loss 1.18 , training accuracy 0.71 , test accuracy 0.61\n",
      "epoch 36 training loss 1.16 , training accuracy 0.71 , test accuracy 0.60\n",
      "epoch 37 training loss 1.15 , training accuracy 0.72 , test accuracy 0.59\n",
      "epoch 38 training loss 1.14 , training accuracy 0.72 , test accuracy 0.61\n",
      "epoch 39 training loss 1.12 , training accuracy 0.72 , test accuracy 0.60\n",
      "epoch 40 training loss 1.13 , training accuracy 0.72 , test accuracy 0.60\n",
      "epoch 41 training loss 1.12 , training accuracy 0.72 , test accuracy 0.61\n",
      "epoch 42 training loss 1.11 , training accuracy 0.73 , test accuracy 0.60\n",
      "epoch 43 training loss 1.10 , training accuracy 0.73 , test accuracy 0.61\n",
      "epoch 44 training loss 1.09 , training accuracy 0.73 , test accuracy 0.62\n",
      "epoch 45 training loss 1.08 , training accuracy 0.73 , test accuracy 0.60\n",
      "epoch 46 training loss 1.08 , training accuracy 0.74 , test accuracy 0.61\n",
      "epoch 47 training loss 1.07 , training accuracy 0.74 , test accuracy 0.60\n",
      "epoch 48 training loss 1.07 , training accuracy 0.74 , test accuracy 0.61\n",
      "epoch 49 training loss 1.06 , training accuracy 0.74 , test accuracy 0.60\n",
      "epoch 50 training loss 1.05 , training accuracy 0.74 , test accuracy 0.63\n",
      "epoch 51 training loss 1.05 , training accuracy 0.74 , test accuracy 0.61\n",
      "epoch 52 training loss 1.04 , training accuracy 0.75 , test accuracy 0.61\n",
      "epoch 53 training loss 1.04 , training accuracy 0.75 , test accuracy 0.62\n",
      "epoch 54 training loss 1.04 , training accuracy 0.75 , test accuracy 0.61\n",
      "epoch 55 training loss 1.03 , training accuracy 0.75 , test accuracy 0.63\n",
      "epoch 56 training loss 1.03 , training accuracy 0.75 , test accuracy 0.61\n",
      "epoch 57 training loss 1.02 , training accuracy 0.75 , test accuracy 0.62\n",
      "epoch 58 training loss 1.02 , training accuracy 0.76 , test accuracy 0.62\n",
      "epoch 59 training loss 1.01 , training accuracy 0.76 , test accuracy 0.63\n",
      "epoch 60 training loss 1.00 , training accuracy 0.76 , test accuracy 0.62\n",
      "epoch 61 training loss 1.00 , training accuracy 0.76 , test accuracy 0.61\n",
      "epoch 62 training loss 1.00 , training accuracy 0.76 , test accuracy 0.61\n",
      "epoch 63 training loss 1.00 , training accuracy 0.76 , test accuracy 0.63\n",
      "epoch 64 training loss 0.99 , training accuracy 0.76 , test accuracy 0.62\n",
      "epoch 65 training loss 0.99 , training accuracy 0.76 , test accuracy 0.62\n",
      "epoch 66 training loss 0.98 , training accuracy 0.76 , test accuracy 0.61\n",
      "epoch 67 training loss 0.99 , training accuracy 0.76 , test accuracy 0.62\n",
      "epoch 68 training loss 0.98 , training accuracy 0.77 , test accuracy 0.62\n",
      "epoch 69 training loss 0.98 , training accuracy 0.76 , test accuracy 0.61\n",
      "epoch 70 training loss 0.98 , training accuracy 0.76 , test accuracy 0.62\n",
      "epoch 71 training loss 0.96 , training accuracy 0.77 , test accuracy 0.62\n",
      "epoch 72 training loss 0.96 , training accuracy 0.77 , test accuracy 0.62\n",
      "epoch 73 training loss 0.96 , training accuracy 0.77 , test accuracy 0.61\n",
      "epoch 74 training loss 0.97 , training accuracy 0.77 , test accuracy 0.62\n",
      "epoch 75 training loss 0.96 , training accuracy 0.77 , test accuracy 0.62\n",
      "epoch 76 training loss 0.95 , training accuracy 0.77 , test accuracy 0.63\n",
      "epoch 77 training loss 0.95 , training accuracy 0.77 , test accuracy 0.62\n",
      "epoch 78 training loss 0.95 , training accuracy 0.77 , test accuracy 0.62\n",
      "epoch 79 training loss 0.95 , training accuracy 0.77 , test accuracy 0.62\n"
     ]
    }
   ],
   "source": [
    "train_single_model(model_4, cifar100_trainloader, cifar100_testloader, num_classes, num_epochs, runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect the train error, test error for the trained single models and store in the '../results/cifar_results.csv' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_model_path = '../models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda:0\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "m = 'cifar100_single_model'\n",
    "plot_file = generate_plot_file(m, specific=str(num_classes)+'_models.pt')\n",
    "models = torch.load(open(os.path.join(pre_trained_model_path, plot_file),'rb'), map_location=device)\n",
    "filename = os.path.join(results_path, 'cifar100_results.csv')\n",
    "if os.path.exists(filename):\n",
    "    p = 'a'\n",
    "else:\n",
    "    p = 'w'\n",
    "        \n",
    "header = ['filename', 'train error', 'test error','mutual information', 'sample entropy', 'experts usage']\n",
    "    \n",
    "with open(filename, p) as f:\n",
    "    writer = csv.writer(f)        \n",
    "\n",
    "    if p == 'w':            \n",
    "        writer.writerow(header)\n",
    "    for i, model in enumerate(models['models']):\n",
    "        data = ['']*5\n",
    "        data[0] = m+'_'+str(i)\n",
    "        running_test_accuracy = 0.0\n",
    "        num_batches = 0\n",
    "        train_error = 1-models['history'][i]['accuracy'][-1]\n",
    "        data[1] = train_error\n",
    "        for test_inputs, test_labels in cifar100_testloader:\n",
    "            test_inputs, test_labels = test_inputs.to(device, non_blocking=True), test_labels.to(device, non_blocking=True)                \n",
    "            outputs = model(test_inputs)\n",
    "            running_test_accuracy += accuracy(outputs, test_labels)\n",
    "            num_batches += 1\n",
    "        test_error = 1-(running_test_accuracy/num_batches)\n",
    "        data[2] = test_error.item()\n",
    "        \n",
    "        writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "mnn",
   "language": "python",
   "name": "mnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
