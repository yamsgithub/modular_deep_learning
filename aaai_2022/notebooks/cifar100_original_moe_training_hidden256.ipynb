{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with CIFAR100 Dataset and Original MoE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiments in this notebook include training the original MoE models as follows:\n",
    "\n",
    "1. original MoE without regularization.\n",
    "2. original MoE with $L_{importance}$ regularization.\n",
    "3. original MoE with $L_s$ regularization.\n",
    "4. train a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import matplotlib.cm as cm  #Â colormaps\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "from math import ceil, sin, cos, radians\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print('device', device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('device', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import MoE expectation model. All experiments for this dataset are done with the expectation model as it\n",
    "# provides the best guarantee of intuitive task decompositions\n",
    "from moe_models.moe_expectation_model import moe_expectation_model\n",
    "from helper.moe_models import cross_entropy_loss\n",
    "from helper.visualise_results import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CIFAR100 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to where the trained models and figures will be stored. You can change this as you see fit.\n",
    "fig_path = '../figures/test'\n",
    "model_path = '../models/hidden_256'\n",
    "pre_trained_model_path = '../models/pre_trained'\n",
    "results_path = '../results'\n",
    "\n",
    "if not os.path.exists(fig_path):\n",
    "    os.mkdir(fig_path)\n",
    "if not os.path.exists(model_path):\n",
    "    os.mkdir(model_path)\n",
    "if not os.path.exists(results_path):\n",
    "    os.mkdir(results_path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = ((0.5074,0.4867,0.4411),(0.2011,0.1987,0.2025))\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32,padding=4,padding_mode=\"reflect\"),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(*stats)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(*stats)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset CIFAR100\n",
       "     Number of datapoints: 10000\n",
       "     Root location: ./data\n",
       "     Split: Test\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "                Normalize(mean=(0.5074, 0.4867, 0.4411), std=(0.2011, 0.1987, 0.2025))\n",
       "            ),\n",
       " Dataset CIFAR100\n",
       "     Number of datapoints: 50000\n",
       "     Root location: ./data\n",
       "     Split: Train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                RandomHorizontalFlip(p=0.5)\n",
       "                RandomCrop(size=(32, 32), padding=4)\n",
       "                ToTensor()\n",
       "                Normalize(mean=(0.5074, 0.4867, 0.4411), std=(0.2011, 0.1987, 0.2025))\n",
       "            ))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar100_trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
    "cifar100_testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)\n",
    "cifar100_testset, cifar100_trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainsize = 50000\n",
    "testsize = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar100_trainloader = torch.utils.data.DataLoader(torch.utils.data.Subset(cifar100_trainset, range(trainsize)), batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2, pin_memory=True)\n",
    "cifar100_testloader = torch.utils.data.DataLoader(torch.utils.data.Subset(cifar100_testset, range(testsize)), batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple',\n",
       " 'aquarium_fish',\n",
       " 'baby',\n",
       " 'bear',\n",
       " 'beaver',\n",
       " 'bed',\n",
       " 'bee',\n",
       " 'beetle',\n",
       " 'bicycle',\n",
       " 'bottle',\n",
       " 'bowl',\n",
       " 'boy',\n",
       " 'bridge',\n",
       " 'bus',\n",
       " 'butterfly',\n",
       " 'camel',\n",
       " 'can',\n",
       " 'castle',\n",
       " 'caterpillar',\n",
       " 'cattle',\n",
       " 'chair',\n",
       " 'chimpanzee',\n",
       " 'clock',\n",
       " 'cloud',\n",
       " 'cockroach',\n",
       " 'couch',\n",
       " 'cra',\n",
       " 'crocodile',\n",
       " 'cup',\n",
       " 'dinosaur',\n",
       " 'dolphin',\n",
       " 'elephant',\n",
       " 'flatfish',\n",
       " 'forest',\n",
       " 'fox',\n",
       " 'girl',\n",
       " 'hamster',\n",
       " 'house',\n",
       " 'kangaroo',\n",
       " 'keyboard',\n",
       " 'lamp',\n",
       " 'lawn_mower',\n",
       " 'leopard',\n",
       " 'lion',\n",
       " 'lizard',\n",
       " 'lobster',\n",
       " 'man',\n",
       " 'maple_tree',\n",
       " 'motorcycle',\n",
       " 'mountain',\n",
       " 'mouse',\n",
       " 'mushroom',\n",
       " 'oak_tree',\n",
       " 'orange',\n",
       " 'orchid',\n",
       " 'otter',\n",
       " 'palm_tree',\n",
       " 'pear',\n",
       " 'pickup_truck',\n",
       " 'pine_tree',\n",
       " 'plain',\n",
       " 'plate',\n",
       " 'poppy',\n",
       " 'porcupine',\n",
       " 'possum',\n",
       " 'rabbit',\n",
       " 'raccoon',\n",
       " 'ray',\n",
       " 'road',\n",
       " 'rocket',\n",
       " 'rose',\n",
       " 'sea',\n",
       " 'seal',\n",
       " 'shark',\n",
       " 'shrew',\n",
       " 'skunk',\n",
       " 'skyscraper',\n",
       " 'snail',\n",
       " 'snake',\n",
       " 'spider',\n",
       " 'squirrel',\n",
       " 'streetcar',\n",
       " 'sunflower',\n",
       " 'sweet_pepper',\n",
       " 'table',\n",
       " 'tank',\n",
       " 'telephone',\n",
       " 'television',\n",
       " 'tiger',\n",
       " 'tractor',\n",
       " 'train',\n",
       " 'trout',\n",
       " 'tulip',\n",
       " 'turtle',\n",
       " 'wardrobe',\n",
       " 'whale',\n",
       " 'willow_tree',\n",
       " 'wolf',\n",
       " 'woman',\n",
       " 'worm']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "with open('data/cifar100_class_names.txt','r') as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=' ')\n",
    "    classes_cifar100 = []\n",
    "    for row in csvreader:\n",
    "        if row:\n",
    "            classes_cifar100.append(row[1])\n",
    "\n",
    "classes_cifar100            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to display the images\n",
    "def plot_colour_images(images_to_plot, titles=None, nrows=None, ncols=6, thefigsize=(18,18)):\n",
    "    # images_to_plot: list of images to be displayed\n",
    "    # titles: list of titles corresponding to the images\n",
    "    # ncols: The number of images per row to display. The number of rows \n",
    "    #        is computed from the number of images to display and the ncols\n",
    "    # theFigsize: The size of the layour of all the displayed images\n",
    "    \n",
    "    n_images = images_to_plot.shape[0]\n",
    "    \n",
    "    # Compute the number of rows\n",
    "    if nrows is None:\n",
    "        nrows = np.ceil(n_images/ncols).astype(int)\n",
    "    \n",
    "    fig,ax = plt.subplots(nrows, ncols, sharex=True, sharey=True, figsize=thefigsize)\n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    for i in range(n_images):\n",
    "        img = images_to_plot[i,:,:,:]\n",
    "        npimg = np.clip(img.numpy(),0,1)\n",
    "        ax[i].imshow(npimg)\n",
    "        ax[i].axis('off')  \n",
    "        if titles is not None and i<10:\n",
    "            ax[i].set_title(titles[i%10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get some random training images\n",
    "# dataiter = iter(cifar100_trainloader)\n",
    "# images, labels = dataiter.next()\n",
    "\n",
    "# images_to_plot = []\n",
    "# count = 0\n",
    "# selected_labels = []\n",
    "# for i in range(100):\n",
    "#     if count == 10:\n",
    "#         break\n",
    "#     index = np.where(labels==i)[0]\n",
    "#     if len(index) >= 3:\n",
    "#         selected_labels.append(i)\n",
    "#         images_to_plot.append(images[index[0:3],:,:])\n",
    "#         count += 1\n",
    "    \n",
    "# selected_labels = [classes_cifar100[i] for i in selected_labels]\n",
    "# images_to_plot = torch.transpose(torch.stack(images_to_plot),0,1)\n",
    "# new_shape = images_to_plot.shape\n",
    "# images_to_plot = images_to_plot.reshape(new_shape[0]*new_shape[1], new_shape[2], new_shape[3], new_shape[4])\n",
    "# images_to_plot = images_to_plot.permute(0,2,3,1)\n",
    "# plot_colour_images(images_to_plot, nrows=3, ncols=10,thefigsize=(20,6), titles=selected_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define expert and gate networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional network with 4 convultional layers and 2 hidden layers with ReLU activation\n",
    "class expert_layers(nn.Module):\n",
    "    def __init__(self, num_classes, channels=3):\n",
    "        super(expert_layers, self).__init__()\n",
    "        filter_size = 3\n",
    "        self.filters = 16\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=self.filters, kernel_size=filter_size, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=self.filters, out_channels=self.filters*2, kernel_size=filter_size, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(self.filters*2)\n",
    "        self.mp = nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels= self.filters*2, out_channels=self.filters*4, kernel_size=filter_size, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=self.filters*4, out_channels=self.filters*8, kernel_size=filter_size, stride=1, padding=1,bias=False)\n",
    "        self.bn8 = nn.BatchNorm2d(self.filters*8)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.filters*8*2*2,1024)\n",
    "        self.fc2 = nn.Linear(1024, 256)\n",
    "        \n",
    "        self.out = nn.Linear(in_features=256, out_features=num_classes)\n",
    "                        \n",
    "    def forward(self, x):\n",
    "        # conv 1        \n",
    "        x = self.mp(F.relu(self.conv1(x)))\n",
    "        x = self.mp(F.relu(self.bn2(self.conv2(x))))    \n",
    "    \n",
    "        x = self.mp(F.relu(self.conv3(x)))\n",
    "        x = self.mp(F.relu(self.bn8(self.conv4(x))))\n",
    "\n",
    "        x = x.reshape(-1, self.filters*8*2*2)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        x = self.out(x)\n",
    "        \n",
    "        # output\n",
    "        x = F.softmax(x, dim=1)\n",
    "\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional network with 4 convultional layers and 2 hidden layers with ReLU activation\n",
    "class gate_layers(nn.Module):\n",
    "    def __init__(self, num_experts):\n",
    "        super(gate_layers, self).__init__()\n",
    "        # define layers\n",
    "        filter_size = 3\n",
    "        self.filters = 64\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=self.filters, kernel_size=filter_size, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=self.filters, out_channels=self.filters*2, kernel_size=filter_size, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(self.filters*2)\n",
    "        self.mp = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels= self.filters*2, out_channels=self.filters*4, kernel_size=filter_size, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=self.filters*4, out_channels=self.filters*8, kernel_size=filter_size, stride=1, padding=1, bias=False)\n",
    "        self.bn8 = nn.BatchNorm2d(self.filters*8)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.filters*8*2*2, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 256)\n",
    "        \n",
    "        self.out = nn.Linear(in_features=256, out_features=num_experts)\n",
    "        \n",
    "    def forward(self, x, T=1.0, y=None):\n",
    "        # conv 1        \n",
    "        x = self.mp(F.relu(self.conv1(x)))\n",
    "        x = self.mp(F.relu(self.bn2(self.conv2(x))))\n",
    "\n",
    "        x = self.mp(F.relu(self.conv3(x)))\n",
    "        x = self.mp(F.relu(self.bn8(self.conv4(x))))\n",
    "                \n",
    "        x = x.reshape(-1, self.filters*8*2*2)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        x = self.out(x)\n",
    "        x = F.softmax(x/T, dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a set of experts\n",
    "def experts(num_experts, num_classes, expert_layers_type=expert_layers):\n",
    "    models = []\n",
    "    for i in range(num_experts):\n",
    "        models.append(expert_layers_type(num_classes))\n",
    "    return nn.ModuleList(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convolutional network with 4 convultionals layers and 2 hidden layers with ReLU activation\n",
    "class single_model(nn.Module):\n",
    "    def __init__(self, num_classes, channels=3):\n",
    "        super(single_model, self).__init__()\n",
    "        filter_size = 3\n",
    "        self.filters = 16\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=self.filters, kernel_size=filter_size, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=self.filters, out_channels=self.filters*2, kernel_size=filter_size, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(self.filters*2)\n",
    "        self.mp = nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels= self.filters*2, out_channels=self.filters*4, kernel_size=filter_size, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=self.filters*4, out_channels=self.filters*8, kernel_size=filter_size, stride=1, padding=1,bias=False)\n",
    "        self.bn8 = nn.BatchNorm2d(self.filters*8)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.filters*8*2*2,1024)\n",
    "        self.fc2 = nn.Linear(1024, 256)\n",
    "        \n",
    "        self.out = nn.Linear(in_features=256, out_features=num_classes)\n",
    "                        \n",
    "    def forward(self, x):\n",
    "        # conv 1        \n",
    "        x = self.mp(F.relu(self.conv1(x)))\n",
    "        x = self.mp(F.relu(self.bn2(self.conv2(x))))    \n",
    "    \n",
    "        x = self.mp(F.relu(self.conv3(x)))\n",
    "        x = self.mp(F.relu(self.bn8(self.conv4(x))))\n",
    "                    \n",
    "        x = x.reshape(-1, self.filters*8*2*2)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        x = self.out(x)\n",
    "        \n",
    "        # output\n",
    "        x = F.softmax(x, dim=1)\n",
    "                \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize configurations and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy of the model\n",
    "def accuracy(out, yb, mean=True):\n",
    "    preds = torch.argmax(out, dim=1).to(device, non_blocking=True)\n",
    "    if mean:\n",
    "        return (preds == yb).float().mean()\n",
    "    else:\n",
    "        return (preds == yb).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to train models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to train original model with and without regularization\n",
    "\n",
    "* w_importance_range is the range of values for the $w_{importance}$ hyperparameter of the $L_{importance}$ regularization.\n",
    "* w_sample_sim_same_range is the range of values for $\\beta_s$ hyperparameter of the $L_s$ regularization.\n",
    "* w_sample_sim_diff_range is the range of values for $\\beta_d$ hyperparameter of the $L_s$ regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def train_original_model(model_1, trainloader, testloader, runs, temps=[[1.0]*20], \n",
    "                         w_importance_range=[0.0], w_sample_sim_same_range=[0.0], \n",
    "                         w_sample_sim_diff_range=[0.0],\n",
    "                         num_classes=10, total_experts=5, num_epochs=20):\n",
    "\n",
    "    for T, w_importance, w_sample_sim_same, w_sample_sim_diff in product(temps, w_importance_range, \n",
    "                                                                         w_sample_sim_same_range,  w_sample_sim_diff_range):\n",
    "        \n",
    "        print('w_importance','{:.1f}'.format(w_importance))\n",
    "        if w_sample_sim_same < 1:\n",
    "            print('w_sample_sim_same',str(w_sample_sim_same))\n",
    "        else:\n",
    "            print('w_sample_sim_same','{:.1f}'.format(w_sample_sim_same))\n",
    "        \n",
    "        if w_sample_sim_diff < 1:\n",
    "            print('w_sample_sim_diff',str(w_sample_sim_diff))\n",
    "        else:\n",
    "            print('w_sample_sim_diff','{:.1f}'.format(w_sample_sim_diff))\n",
    "\n",
    "        \n",
    "        for run in range(1, runs+1):\n",
    "            \n",
    "            print('Run:', run)\n",
    "            \n",
    "            n_run_models_1 = []\n",
    "            \n",
    "            models = {'moe_expectation_model':{'model':moe_expectation_model,'loss':cross_entropy_loss().to(device),\n",
    "                                               'experts':{}},}\n",
    "            for key, val in models.items():\n",
    "\n",
    "                expert_models = experts(total_experts, num_classes).to(device)\n",
    "\n",
    "                gate_model = gate_layers(total_experts).to(device)\n",
    "\n",
    "                moe_model = val['model'](total_experts, num_classes,\n",
    "                                         experts=expert_models, gate=gate_model,device=device).to(device)\n",
    "                \n",
    "                optimizer_moe = optim.Adam(moe_model.parameters(), lr=0.001, amsgrad=False, weight_decay=1e-3)\n",
    "                \n",
    "               \n",
    "                hist = moe_model.train(trainloader, testloader,  val['loss'], optimizer_moe = optimizer_moe,\n",
    "                                       T = T, w_importance=w_importance, w_sample_sim_same = w_sample_sim_same, \n",
    "                                       w_sample_sim_diff = w_sample_sim_diff, \n",
    "                                       accuracy=accuracy, epochs=num_epochs)\n",
    "                val['experts'][total_experts] = {'model':moe_model, 'history':hist}                \n",
    "\n",
    "\n",
    "            # Save all the trained models\n",
    "            plot_file = generate_plot_file(model_1, T[0], w_importance=w_importance, w_sample_sim_same=w_sample_sim_same,w_sample_sim_diff=w_sample_sim_diff,\n",
    "                                           specific=str(num_classes)+'_'+str(total_experts)+'_models.pt')\n",
    "            \n",
    "            if os.path.exists(os.path.join(model_path, plot_file)):\n",
    "                n_run_models_1 = torch.load(open(os.path.join(model_path, plot_file),'rb'))\n",
    "            n_run_models_1.append(models)\n",
    "            torch.save(n_run_models_1,open(os.path.join(model_path, plot_file),'wb'))\n",
    "            n_run_models_1 = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to train the single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_single_model(model_name, trainloader, testloader, num_classes, num_epochs, runs):\n",
    "    \n",
    "    loss_criterion = cross_entropy_loss()\n",
    "    \n",
    "    n_runs = {'models':[], 'history':[]}\n",
    "    \n",
    "    for run in range(1, runs+1):\n",
    "        \n",
    "        print('Run', run)\n",
    "        \n",
    "        model = single_model(num_classes).to(device)\n",
    "        history = {'loss':[], 'accuracy':[], 'val_accuracy':[]}\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001, amsgrad=False, weight_decay=1e-3)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0.0\n",
    "            train_running_accuracy = 0.0\n",
    "            num_batches = 0\n",
    "\n",
    "            for inputs, labels in trainloader:\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss = loss_criterion(outputs, None, None, labels)\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                acc = accuracy(outputs, labels)\n",
    "                train_running_accuracy += acc\n",
    "\n",
    "                num_batches += 1\n",
    "\n",
    "            test_running_accuracy = 0.0\n",
    "            test_num_batches = 0\n",
    "            \n",
    "            for test_inputs, test_labels in testloader:\n",
    "                test_inputs, test_labels = test_inputs.to(device, non_blocking=True), test_labels.to(device, non_blocking=True)\n",
    "                test_outputs = model(test_inputs)              \n",
    "                test_running_accuracy += accuracy(test_outputs, test_labels)\n",
    "                test_num_batches += 1\n",
    "                \n",
    "            loss = (running_loss/num_batches)\n",
    "            train_accuracy = (train_running_accuracy/num_batches)\n",
    "            test_accuracy = (test_running_accuracy/test_num_batches)\n",
    "            \n",
    "            history['loss'].append(loss)\n",
    "            history['accuracy'].append(train_accuracy.item())\n",
    "            history['val_accuracy'].append(test_accuracy.item())\n",
    "            \n",
    "            print('epoch %d' % epoch,\n",
    "                  'training loss %.2f' % loss,\n",
    "                   ', training accuracy %.2f' % train_accuracy,\n",
    "                   ', test accuracy %.2f' % test_accuracy\n",
    "                   )\n",
    "            \n",
    "        plot_file = generate_plot_file(model_name, specific=str(num_classes)+'_models.pt')\n",
    "        if os.path.exists(os.path.join(model_path, plot_file)):\n",
    "            n_runs = torch.load(open(os.path.join(model_path, plot_file),'rb'))\n",
    "        n_runs['models'].append(model)\n",
    "        n_runs['history'].append(history)        \n",
    "        torch.save(n_runs, open(os.path.join(model_path, plot_file),'wb'))\n",
    "        \n",
    "        n_runs = {'models':[], 'history':[]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Experiment 1: Original MoE model trained without gate regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with gate and expert parameters initialized to default values\n",
    "model_1 = 'cifar100_without_reg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_experts = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = [[1.0]*num_epochs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_original_model(model_1, cifar100_trainloader, cifar100_testloader, runs, temps=temps,\n",
    "                     num_classes=num_classes, total_experts=total_experts, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Experiment 2: Original MoE model trained with $L_{importance}$ regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with gate and expert parameters initialized to default values\n",
    "model_2 = 'cifar100_with_reg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_experts = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_importance_range =  ['0.2', '0.4', '0.6', '0.8', '1.0']\n"
     ]
    }
   ],
   "source": [
    "w_importance_range = [i * 0.2 for i in range(1, 6)]\n",
    "print('w_importance_range = ', ['{:.1f}'.format(w) for w in w_importance_range])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = [[1]*num_epochs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_original_model(model_2, cifar100_trainloader, cifar100_testloader, runs, temps=temps,\n",
    "                     w_importance_range=w_importance_range, num_classes=num_classes, \n",
    "                     total_experts=total_experts, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Experiment 3: Original MoE model trained with sample similarity regularization, $L_s$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = 'cifar100_with_reg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_experts = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = [[1]*num_epochs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_sample_sim_same_range =  [1e-05, 0.0001, 0.001]\n",
      "w_sample_sim_diff_range =  [1e-07, 1e-06, 1e-05, 0.0001, 0.001, 0.01, 0.1]\n"
     ]
    }
   ],
   "source": [
    "w_sample_sim_same_range = [1e-5,1e-4,1e-3]\n",
    "w_sample_sim_diff_range = [1e-7,1e-6,1e-5, 1e-4, 1e-3,1e-2,1e-1]\n",
    "print('w_sample_sim_same_range = ', w_sample_sim_same_range)\n",
    "print('w_sample_sim_diff_range = ', w_sample_sim_diff_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_original_model(model_3, cifar100_trainloader, cifar100_testloader, runs, temps=temps,\n",
    "                     w_sample_sim_same_range=w_sample_sim_same_range, \n",
    "                     w_sample_sim_diff_range=w_sample_sim_diff_range, \n",
    "                     num_classes=num_classes, total_experts=total_experts, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Experiment 4: Training the single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = 'cifar100_single_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1\n",
      "epoch 0 training loss 4.09 , training accuracy 0.07 , test accuracy 0.12\n",
      "epoch 1 training loss 3.56 , training accuracy 0.15 , test accuracy 0.18\n",
      "epoch 2 training loss 3.35 , training accuracy 0.19 , test accuracy 0.21\n",
      "epoch 3 training loss 3.20 , training accuracy 0.22 , test accuracy 0.23\n",
      "epoch 4 training loss 3.10 , training accuracy 0.24 , test accuracy 0.26\n",
      "epoch 5 training loss 3.00 , training accuracy 0.26 , test accuracy 0.27\n",
      "epoch 6 training loss 2.93 , training accuracy 0.27 , test accuracy 0.29\n",
      "epoch 7 training loss 2.86 , training accuracy 0.28 , test accuracy 0.31\n",
      "epoch 8 training loss 2.80 , training accuracy 0.30 , test accuracy 0.31\n",
      "epoch 9 training loss 2.75 , training accuracy 0.31 , test accuracy 0.32\n",
      "epoch 10 training loss 2.70 , training accuracy 0.32 , test accuracy 0.33\n",
      "epoch 11 training loss 2.67 , training accuracy 0.32 , test accuracy 0.33\n",
      "epoch 12 training loss 2.63 , training accuracy 0.33 , test accuracy 0.34\n",
      "epoch 13 training loss 2.61 , training accuracy 0.34 , test accuracy 0.35\n",
      "epoch 14 training loss 2.56 , training accuracy 0.35 , test accuracy 0.34\n",
      "epoch 15 training loss 2.54 , training accuracy 0.35 , test accuracy 0.36\n",
      "epoch 16 training loss 2.51 , training accuracy 0.36 , test accuracy 0.36\n",
      "epoch 17 training loss 2.49 , training accuracy 0.36 , test accuracy 0.37\n",
      "epoch 18 training loss 2.47 , training accuracy 0.37 , test accuracy 0.38\n",
      "epoch 19 training loss 2.45 , training accuracy 0.37 , test accuracy 0.38\n",
      "epoch 20 training loss 2.43 , training accuracy 0.38 , test accuracy 0.38\n",
      "epoch 21 training loss 2.42 , training accuracy 0.38 , test accuracy 0.39\n",
      "epoch 22 training loss 2.40 , training accuracy 0.38 , test accuracy 0.39\n",
      "epoch 23 training loss 2.39 , training accuracy 0.38 , test accuracy 0.39\n",
      "epoch 24 training loss 2.37 , training accuracy 0.39 , test accuracy 0.38\n",
      "epoch 25 training loss 2.35 , training accuracy 0.39 , test accuracy 0.40\n",
      "epoch 26 training loss 2.34 , training accuracy 0.39 , test accuracy 0.40\n",
      "epoch 27 training loss 2.33 , training accuracy 0.40 , test accuracy 0.40\n",
      "epoch 28 training loss 2.31 , training accuracy 0.40 , test accuracy 0.40\n",
      "epoch 29 training loss 2.30 , training accuracy 0.41 , test accuracy 0.42\n",
      "epoch 30 training loss 2.29 , training accuracy 0.41 , test accuracy 0.40\n",
      "epoch 31 training loss 2.28 , training accuracy 0.41 , test accuracy 0.42\n",
      "epoch 32 training loss 2.27 , training accuracy 0.41 , test accuracy 0.41\n",
      "epoch 33 training loss 2.26 , training accuracy 0.41 , test accuracy 0.42\n",
      "epoch 34 training loss 2.25 , training accuracy 0.42 , test accuracy 0.42\n",
      "epoch 35 training loss 2.24 , training accuracy 0.42 , test accuracy 0.42\n",
      "epoch 36 training loss 2.23 , training accuracy 0.42 , test accuracy 0.42\n",
      "epoch 37 training loss 2.22 , training accuracy 0.42 , test accuracy 0.43\n",
      "epoch 38 training loss 2.22 , training accuracy 0.42 , test accuracy 0.43\n",
      "epoch 39 training loss 2.20 , training accuracy 0.43 , test accuracy 0.42\n"
     ]
    }
   ],
   "source": [
    "train_single_model(model_4, cifar100_trainloader, cifar100_testloader, num_classes, num_epochs, runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect the train error, test error for the trained single models and store in the '../results/cifar100_results.csv' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_model_path = '../models/hidden_256'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file = 'cifar100_results_hidden_256.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "m = 'cifar100_single_model'\n",
    "plot_file = generate_plot_file(m, specific=str(num_classes)+'_models.pt')\n",
    "models = torch.load(open(os.path.join(pre_trained_model_path, plot_file),'rb'), map_location=device)\n",
    "filename = os.path.join(results_path, results_file)\n",
    "if os.path.exists(filename):\n",
    "    p = 'a'\n",
    "else:\n",
    "    p = 'w'\n",
    "        \n",
    "header = ['filename', 'train error', 'test error','mutual information', 'sample entropy', 'experts usage']\n",
    "    \n",
    "with open(filename, p) as f:\n",
    "    writer = csv.writer(f)        \n",
    "\n",
    "    if p == 'w':            \n",
    "        writer.writerow(header)\n",
    "    for i, model in enumerate(models['models']):\n",
    "        data = ['']*5\n",
    "        data[0] = m+'_'+str(i)\n",
    "        running_test_accuracy = 0.0\n",
    "        num_batches = 0\n",
    "        train_error = 1-models['history'][i]['accuracy'][-1]\n",
    "        data[1] = train_error\n",
    "        for test_inputs, test_labels in cifar100_testloader:\n",
    "            test_inputs, test_labels = test_inputs.to(device, non_blocking=True), test_labels.to(device, non_blocking=True)                \n",
    "            outputs = model(test_inputs)\n",
    "            running_test_accuracy += accuracy(outputs, test_labels)\n",
    "            num_batches += 1\n",
    "        test_error = 1-(running_test_accuracy/num_batches)\n",
    "        data[2] = test_error.item()\n",
    "        \n",
    "        writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "mnn",
   "language": "python",
   "name": "mnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
