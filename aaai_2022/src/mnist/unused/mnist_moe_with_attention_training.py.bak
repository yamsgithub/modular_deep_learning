#!/usr/bin/env python
# coding: utf-8

# # Experiments with MNIST Dataset and Original MoE

# The experiments in this notebook include training the original MoE models as follows:
# 
# 1. original MoE without regularization.
# 2. original MoE with $L_{importance}$ regularization.
# 3. original MoE with $L_s$ regularization.
# 4. train a single model.
import time
import numpy as np
from statistics import mean
from math import ceil, sin, cos, radians
from collections import OrderedDict
import os
from itertools import product

import torch
import torchvision
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import TensorDataset
import torchvision.transforms.functional as TF

# import MoE expectation model. All experiments for this dataset are done with the expectation model as it
# provides the best performance
from moe_models.moe_expectation_model import moe_expectation_model
from moe_models.moe_stochastic_model import moe_stochastic_model
from moe_models.moe_top_k_model import moe_top_k_model
from moe_models.moe_models_base import default_optimizer
from helper.moe_models import cross_entropy_loss, stochastic_loss
from helper.visualise_results import *


# ### NOTE: Pre-trained models are provided to check the results of all the experiments if you do not have the time to train all the models. 

# ## Load MNIST dataset

import torchvision.transforms as transforms

# transforms: Convert PIL image to tensors and normalize
mnist_transform = transforms.Compose(
    [transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))]) #mean and standard deviation computed from the dataset

# Complete train and test data
trainsize = 60000
testsize = 10000

batch_size = 512

# Load data as train and test
trainset = torchvision.datasets.MNIST('./data',
    download=True,
    train=True,
    transform=mnist_transform,
    target_transform = torch.tensor,                                 
    )
testset = torchvision.datasets.MNIST('./data',
    download=True,
    train=False,
    transform=mnist_transform,
    target_transform = torch.tensor,)

# dataloaders
trainloader = torch.utils.data.DataLoader(torch.utils.data.Subset(trainset, range(trainsize)), 
                                          batch_size=batch_size,
                                          shuffle=True)

testloader = torch.utils.data.DataLoader(torch.utils.data.Subset(testset, range(testsize)),
                                         batch_size=testsize,
                                         shuffle=False)
num_classes = 10

image, label = trainset.__getitem__(0)
print('Image shape', image.shape)
print('Train samples ', len(trainset))
print('Test samples ', len(testset))


# ## Define expert and gate networks

# Convolutional network with one convultional layer and 2 hidden layers with ReLU activation
# Convolutional network with one convultional layer and 2 hidden layers with ReLU activation
class expert_layers(nn.Module):
    def __init__(self, num_classes, channels=1):
        super(expert_layers, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=channels, out_channels=1, kernel_size=3)
        self.fc1 = nn.Linear(in_features=1*13*13, out_features=5) # this is a pure linear transform
        self.fc2 = nn.Linear(in_features=5, out_features=32) # this is a pure linear transform
        
        self.out = nn.Linear(in_features=32, out_features=num_classes)
                
    def forward(self, t):
        # conv 1
        t = self.conv1(t)
        # print('conv1', t.shape)
        t = F.relu(t)
        t = F.max_pool2d(t, kernel_size=2, stride=2)
        # print('max pool', t.shape)
        # fc1
        t = t.reshape(-1, 1*13*13)
        t = self.fc1(t)
        t = F.relu(t)

        # fc2
        t = self.fc2(t)
                      
        self.hidden = t
            
        t = F.relu(t)
                
        # output
        t = F.softmax(self.out(t), dim=1) # The temperature parameter is 1 for all the experiments
        
        return t

# Convolutional network with one convultional layer and 2 hidden layers with ReLU activation
class gate_layers(nn.Module):
    def __init__(self, num_experts):
        super(gate_layers, self).__init__()
        # define layers
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3)
        self.fc1 = nn.Linear(in_features=1*13*13, out_features=128)
        self.fc2 = nn.Linear(in_features=128, out_features=32)
        self.mp = nn.MaxPool2d(2,2)
        self.out = nn.Linear(in_features=32, out_features=num_experts)
        self.num_experts = num_experts

    def forward(self, t, T=1.0, y=None):
        # conv 1
        t = self.mp(F.relu(self.conv1(t)))
        
        # fc1
        t = t.reshape(-1, 1*13*13)
            
        t = F.relu(self.fc1(t))

        # fc2
        t = F.relu(self.fc2(t))
        
        # output expert log loss
        t = self.out(t)
        
        output = F.softmax(t/T, dim=1)

        return output


# Convolutional network with one convultional layer and 2 hidden layers with ReLU activation
class gate_layers_top_k(nn.Module):
    def __init__(self, num_experts):
        super(gate_layers_top_k, self).__init__()
        # define layers
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3)
        self.fc1 = nn.Linear(in_features=1*13*13, out_features=128)
        self.fc2 = nn.Linear(in_features=128, out_features=32)
        self.mp = nn.MaxPool2d(2,2)
        self.out = nn.Linear(in_features=32, out_features=num_experts)
        self.num_experts = num_experts

    def forward(self, t, T=1.0, y=None):
        # conv 1
        t = self.mp(F.relu(self.conv1(t)))
        
        # fc1
        t = t.reshape(-1, 1*13*13)
            
        t = F.relu(self.fc1(t))

        # fc2
        t = F.relu(self.fc2(t))
        
        # output expert log loss
        t = self.out(t)
        
        output = t/T

        return output
    
# Convolutional network with one convolutional layer and 2 hidden layers with ReLU activation
class gate_attn_layers(nn.Module):
    def __init__(self, num_experts, channels=1):
        super(gate_attn_layers, self).__init__()
        # define layers
        self.conv1 = nn.Conv2d(in_channels=channels, out_channels=1, kernel_size=3)

        self.fc1 = nn.Linear(in_features=1*13*13, out_features=128)
        self.fc2 = nn.Linear(in_features=128, out_features=32)
                
    def forward(self, t, T=1.0, y=None):
        # conv 1
        t = self.conv1(t)
        t = F.relu(t)
        t = F.max_pool2d(t, kernel_size=2, stride=2)
        # fc1
        t = t.reshape(-1, 1*13*13)
            
        t = self.fc1(t)
        t = F.relu(t)

        # fc2
        t = self.fc2(t)
        
        return t


# Convolutional network with one convultional layer and 2 hidden layers with ReLU activation. The single model has the same
# architecture as an expert
class single_model(nn.Module):
    def __init__(self, num_classes=10):
        super(single_model, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3)
        self.fc1 = nn.Linear(in_features=1*13*13, out_features=5) # this is a pure linear transform
        self.fc2 = nn.Linear(in_features=5, out_features=32) # this is a pure linear transform
        
        self.out = nn.Linear(in_features=32, out_features=num_classes)
        
        self.num_classes = num_classes
        
    def forward(self, t):
        # conv 1
        t = self.conv1(t)
        
        t = F.relu(t)
        t = F.max_pool2d(t, kernel_size=2, stride=2)
        # fc1
        t = t.reshape(-1, 1*13*13)
        t = self.fc1(t)
        t = F.relu(t)

        # fc2
        t = self.fc2(t)
        t = F.relu(t)

        # output
        t = F.softmax(self.out(t), dim=1)
        
        return t


# ## Functions to train models

# ### Function to train original model with and without regularization
# 
# * w_importance_range is the range of values for the $w_{importance}$ hyperparameter of the $L_{importance}$ regularization.
# * w_sample_sim_same_range is the range of values for $\beta_s$ hyperparameter of the $L_s$ regularization.
# * w_sample_sim_diff_range is the range of values for $\beta_d$ hyperparameter of the $L_s$ regularization.



# ### Function to train the single model

# In[19]:


# def train_single_model(model_name, trainloader, testloader, num_classes, num_epochs, runs):
    
#     loss_criterion = cross_entropy_loss()
    
#     n_runs = {'models':[], 'history':[]}
    
#     for run in range(1, runs+1):
        
#         print('Run', run)
        
#         model = single_model(num_classes).to(device)
#         history = {'loss':[], 'accuracy':[], 'val_accuracy':[]}
#         optimizer = optim.Adam(model.parameters(), lr=0.001, amsgrad=False)
        
#         for epoch in range(num_epochs):
#             running_loss = 0.0
#             train_running_accuracy = 0.0
#             num_batches = 0

#             for inputs, labels in trainloader:
#                 inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)
#                 outputs = model(inputs)

#                 optimizer.zero_grad()
#                 loss = loss_criterion(outputs, None, None, labels)

#                 loss.backward()

#                 optimizer.step()

#                 running_loss += loss.item()

#                 outputs = model(inputs)

#                 acc = accuracy(outputs, labels)
#                 train_running_accuracy += acc

#                 num_batches += 1

#             test_running_accuracy = 0.0
#             test_num_batches = 0
            
#             for test_inputs, test_labels in testloader:
#                 test_inputs, test_labels = test_inputs.to(device, non_blocking=True), test_labels.to(device, non_blocking=True)
#                 test_outputs = model(test_inputs)              
#                 test_running_accuracy += accuracy(test_outputs, test_labels)
#                 test_num_batches += 1
                
#             loss = (running_loss/num_batches)
#             train_accuracy = (train_running_accuracy/num_batches)
#             test_accuracy = (test_running_accuracy/test_num_batches)
            
#             history['loss'].append(loss)
#             history['accuracy'].append(train_accuracy.item())
#             history['val_accuracy'].append(test_accuracy.item())
            
#             print('epoch %d' % epoch,
#                   'training loss %.2f' % loss,
#                    ', training accuracy %.2f' % train_accuracy,
#                    ', test accuracy %.2f' % test_accuracy
#                    )
        
#         # Save all the trained models
#         plot_file = generate_plot_file(model_name, specific=str(num_classes)+'_models.pt')
#         if  os.path.exists(os.path.join(model_path, plot_file)):
#             n_runs = torch.load(open(os.path.join(model_path, plot_file),'rb'))
#         n_runs['models'].append(model)
#         n_runs['history'].append(history)        
#         torch.save(n_runs, open(os.path.join(model_path, plot_file),'wb'))
        
#         n_runs = {'models':[], 'history':[]}


